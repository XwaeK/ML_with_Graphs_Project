{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required packages.\n",
    "# import os\n",
    "# import torch\n",
    "# os.environ['TORCH'] = torch.__version__\n",
    "# print(torch.__version__)\n",
    "\n",
    "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import tracemalloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_graph_edges(H, W, k=5):\n",
    "    \"\"\"\n",
    "    Generates an edges tensor for a graph where each pixel in an HxW image\n",
    "    is connected to its kxk nearest neighbors, shifting the window at borders.\n",
    "\n",
    "    Args:\n",
    "    - H (int): Image height\n",
    "    - W (int): Image width\n",
    "    - k (int): Neighborhood size (kxk)\n",
    "    - device (str): 'cuda' or 'cpu' for tensor allocation\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Edges tensor of shape (num_edges, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    half_k = k // 2\n",
    "\n",
    "    # Create grid of pixel indices\n",
    "    row_indices = torch.arange(H)\n",
    "    col_indices = torch.arange(W)\n",
    "\n",
    "    grid_r, grid_c = torch.meshgrid(row_indices, col_indices, indexing='ij')\n",
    "    pixel_indices = grid_r * W + grid_c  # Convert (row, col) to 1D index\n",
    "\n",
    "    # Create all possible shifts within the kxk neighborhood\n",
    "    d_row = torch.arange(-half_k, half_k + 1)\n",
    "    d_col = torch.arange(-half_k, half_k + 1)\n",
    "\n",
    "    shift_r, shift_c = torch.meshgrid(d_row, d_col, indexing='ij')\n",
    "    shift_r = shift_r.flatten()\n",
    "    shift_c = shift_c.flatten()\n",
    "\n",
    "    # Compute neighbor locations\n",
    "    neighbor_r = grid_r.unsqueeze(-1) + shift_r\n",
    "    neighbor_c = grid_c.unsqueeze(-1) + shift_c\n",
    "\n",
    "    # Shift window for boundary pixels\n",
    "    neighbor_r = neighbor_r.clamp(0, H - 1)\n",
    "    neighbor_c = neighbor_c.clamp(0, W - 1)\n",
    "\n",
    "    # Convert to 1D indices\n",
    "    neighbor_indices = neighbor_r * W + neighbor_c\n",
    "\n",
    "    # Create edges\n",
    "    edges = torch.stack([\n",
    "        pixel_indices.unsqueeze(-1).expand(-1, -1, k * k),  # Expand for each neighbor\n",
    "        neighbor_indices\n",
    "    ], dim=-1)\n",
    "\n",
    "    return edges.reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.sparse\n",
    "import gc\n",
    "\n",
    "class GraphLearning(nn.Module):\n",
    "    def __init__(self, feature_dim, edges_p_node, device='cuda'):\n",
    "        super(GraphLearning, self).__init__()\n",
    "        self.M = nn.Parameter(torch.eye(feature_dim, feature_dim, device=device) * 1.5)\n",
    "        self.edges_p_node = edges_p_node\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, edges):\n",
    "        \"\"\"\n",
    "        let B = batch_size, N = num_nodes, M = num_edges, K = sample_size, C = num_channels, F = feature_dim\n",
    "        x: tensor of features for each graph (B, N, F)\n",
    "        edges: tensor of edge indices for each graph (B, M, 2)\n",
    "        return: incidence matrix C for each graph (B, N, M)\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes, _ = x.shape\n",
    "        num_edges = edges.shape[1]\n",
    "\n",
    "        device = self.device\n",
    "\n",
    "        source_x = x[torch.arange(batch_size).unsqueeze(1), edges[:, :, 0], :] # source_x (B, M, F)\n",
    "        target_x = x[torch.arange(batch_size).unsqueeze(1), edges[:, :, 1], :] # target_x (B, M, F)\n",
    "        d = target_x - source_x # d (B, M, F)\n",
    "        e = torch.einsum(\"bik,kl,bil->bi\", d, self.M, d) # e (B, M)\n",
    "\n",
    "        batch_indices = torch.arange(batch_size).unsqueeze(1).repeat(2, num_edges).view(-1)\n",
    "        # batch_indices (B * 2M, )\n",
    "        row_indices = torch.cat([edges[:, :, 0], edges[:, :, 1]]).view(-1)\n",
    "        # row_indices (B * 2M, )\n",
    "        col_indices = torch.cat([torch.arange(num_edges)] * 2).repeat(batch_size,1).view(-1)\n",
    "        # col_indices (B * 2M, )\n",
    "        idxs = torch.stack([batch_indices.to(device), row_indices, col_indices.to(device)], dim=0)\n",
    "        # idxs (3, B * 2M)\n",
    "\n",
    "        vals = torch.cat([e, -e]).view(-1) # vals (B * 2M, )\n",
    "        vals = softmax(vals, row_indices) # vals (B * 2M, )\n",
    "\n",
    "        C = torch.sparse_coo_tensor(idxs, vals, size=(batch_size, num_nodes, num_edges), device=device, check_invariants=True).coalesce()\n",
    "        # C = C.to_sparse_csr()\n",
    "        with torch.no_grad():\n",
    "            del batch_indices, row_indices, col_indices, idxs, vals, source_x, target_x, d, e\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # C = torch.sparse.softmax(C, dim=2)\n",
    "        C = C.to_sparse_coo()\n",
    "        # C sparse(B, M, N)\n",
    "        return C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.sparse\n",
    "\n",
    "class SparseBMM(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, M1, M2):\n",
    "        assert len(M1.shape) == 3 and len(M2.shape) == 3\n",
    "        B = M1.shape[0]\n",
    "        \n",
    "        # Perform batch sparse-dense matrix multiplication\n",
    "        O = torch.stack([torch.sparse.mm(M1[i], M2[i]) for i in range(B)], dim=0)\n",
    "\n",
    "        # Save tensors for backward\n",
    "        ctx.save_for_backward(M1, M2)\n",
    "        return O\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        M1, M2 = ctx.saved_tensors\n",
    "        B = M1.shape[0]\n",
    "        grad = grad_output.to_sparse_coo()\n",
    "        # grad = grad_output\n",
    "        \n",
    "        # Compute gradients for sparse M1 and dense M2\n",
    "        grad_M1 = torch.stack([torch.sparse.mm(grad[i], M2[i].T) for i in range(B)], dim=0)\n",
    "        grad_M2 = torch.stack([torch.sparse.mm(M1[i].T, grad[i]) for i in range(B)], dim=0)\n",
    "        # if M1.is_sparse:\n",
    "        #     grad_M1 = grad_M1.to_sparse_coo()\n",
    "        # else:\n",
    "        #     grad_M1 = grad_M1.to_dense()\n",
    "        # if M2.is_sparse:\n",
    "        #     grad_M2 = grad_M2.to_sparse_coo()\n",
    "        # else:\n",
    "        #     grad_M2 = grad_M2.to_dense()\n",
    "        return grad_M1, grad_M2\n",
    "\n",
    "# Wrapper function\n",
    "def my_bmm(M1, M2):\n",
    "    return SparseBMM.apply(M1, M2)\n",
    "\n",
    "class SparseCache:\n",
    "    eyes_c = {}\n",
    "    zeros_c = {}\n",
    "    device = 'cpu'\n",
    "    @staticmethod\n",
    "    def set_device(device):\n",
    "        if(SparseCache.device != device):\n",
    "            SparseCache.eyes_c = {}\n",
    "            SparseCache.zeros_C = {}\n",
    "        SparseCache.device = device\n",
    "\n",
    "    @staticmethod\n",
    "    def _sparse_eye(S):\n",
    "        return torch.sparse.spdiags(torch.ones(S),torch.zeros(1, dtype=int),(S,S)).coalesce().to(SparseCache.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def eye_(S):\n",
    "        SparseCache.eyes_c[(S,S)] = SparseCache.eyes_c.get((S,S),SparseCache._sparse_eye(S))\n",
    "        return SparseCache.eyes_c[(S,S)]\n",
    "\n",
    "    def eye(B,S):\n",
    "        SparseCache.eyes_c[(B,S,S)] = SparseCache.eyes_c.get((B,S,S),torch.stack([SparseCache.eye_(S) for _ in range(B)], dim=0).coalesce().detach())\n",
    "        return SparseCache.eyes_c[(B,S,S)]\n",
    "\n",
    "    def zeros(S):\n",
    "        SparseCache.zeros_c[S] = SparseCache.zeros_c.get(S,torch.sparse_coo_tensor(size=S, device=SparseCache.device).coalesce().detach())\n",
    "        return SparseCache.zeros_c[S]\n",
    "\n",
    "def sparse_block_matrix(blocks:list[list[torch.Tensor]],row_dim,col_dim):\n",
    "    nnz = 0\n",
    "    row_dims = []\n",
    "    row_col_dims = []\n",
    "    for row in blocks:\n",
    "        col_dims = []\n",
    "        row_d = row[0].shape[row_dim]\n",
    "        for block in row:\n",
    "            assert block.shape[row_dim] == row_d, \"All blocks in row must have same row dim\"\n",
    "            col_d = block.shape[col_dim]\n",
    "            nnz += block._nnz()\n",
    "            col_dims.append(col_d)\n",
    "        row_dims.append(row_d)\n",
    "        row_col_dims.append(col_dims)\n",
    "        assert sum(col_dims) == sum(row_col_dims[0]), \"Must have consistent number of columns\"\n",
    "    num_dims = len(blocks[0][0].shape)\n",
    "    device = blocks[0][0].device\n",
    "    idxs = torch.empty((num_dims,nnz))\n",
    "    vals = torch.empty((nnz,))\n",
    "    idx_offset = 0\n",
    "    row_offset = 0\n",
    "    for row_i, row in enumerate(blocks):\n",
    "        col_offset = 0\n",
    "        col_dims = row_col_dims[row_i]\n",
    "        for col_i, block in enumerate(row):\n",
    "            block_idxs = block.indices().clone().detach()\n",
    "            block_vals = block.values()\n",
    "            # check submatrix validity\n",
    "            # _CHECK = torch.sparse_coo_tensor(block_idxs, block_vals, size=block.shape, check_invariants=True).coalesce()\n",
    "            row_idx = block_idxs[row_dim] + row_offset\n",
    "            col_idx = block_idxs[col_dim] + col_offset\n",
    "            block_idxs[row_dim] = row_idx\n",
    "            block_idxs[col_dim] = col_idx\n",
    "            nnz = block._nnz()\n",
    "            idxs[:,idx_offset:idx_offset+nnz] = block_idxs\n",
    "            vals[idx_offset:idx_offset+nnz] = block_vals\n",
    "            del block_idxs, row_idx, col_idx\n",
    "            idx_offset += nnz\n",
    "\n",
    "            col_offset += col_dims[col_i]\n",
    "        row_offset += row_dims[row_i]\n",
    "    out_size = list(blocks[0][0].shape)\n",
    "    out_size[row_dim] = sum(row_dims)\n",
    "    out_size[col_dim] = sum(row_col_dims[0])\n",
    "    out = torch.sparse_coo_tensor(idxs, vals,size=tuple(out_size),device=device,check_invariants=True).coalesce()\n",
    "    del idxs, vals, out_size\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class ADMMLayer(nn.Module):\n",
    "    def __init__(self, cg_steps, edges_p_node, sample_size, device):\n",
    "        super(ADMMLayer, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.rand(1, device=device))\n",
    "        self.alpha = nn.Parameter(torch.rand(cg_steps, device=device))\n",
    "        self.beta = nn.Parameter(torch.rand(cg_steps, device=device))\n",
    "        self.cg_steps = cg_steps\n",
    "        self.edges_p_node = edges_p_node\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "        self.device = device\n",
    "        SparseCache.set_device(device)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, C, H, y, x, q_tilde, mu):\n",
    "        \"\"\"\n",
    "        let B = batch_size, N = num_nodes, M = num_edges, K = sample_size, C = num_channels\n",
    "        C: incidence matrix of size (B, N, M)\n",
    "        H: sampling matrix of size (B, K, N)\n",
    "        y: signal to interpolate of size (B, K, C)\n",
    "        x: initial guess of size (B, N, C)\n",
    "        q_tilde: initial guess of size (B, 2M, C)\n",
    "        mu: initial guess of size (B, 4M + K, C)\n",
    "        \"\"\"\n",
    "        K = self.sample_size\n",
    "        B, N, channels = x.shape\n",
    "        _, _, M = C.shape\n",
    "        gamma = self.gamma\n",
    "        device = C.device\n",
    "        mu_a, mu_b, mu_c, mu_d, mu_e = mu.narrow(1, 0, M), mu.narrow(1, M, M), \\\n",
    "                                       mu.narrow(1, 2*M, K), mu.narrow(1, 2*M + K, M), \\\n",
    "                                       mu.narrow(1, 3*M + K, M)\n",
    "        # mu_a (B,M,C), mu_b (B,M,C), mu_c (B,K,C), mu_d (B,M,C), mu_e (B,M,C)\n",
    "\n",
    "        q_tilde_1, q_tilde_2 = q_tilde.narrow(1, 0, M), q_tilde.narrow(1, M, M)\n",
    "        # q_tilde_1 (B,M,C), q_tilde_2 (B,M,C)\n",
    "\n",
    "        z_n = -1 / gamma * torch.ones(B,M,channels, device=device) \\\n",
    "              - 1 / (2 * gamma) * (mu_a + mu_b + mu_d + mu_e) \\\n",
    "              + 1 / 2 * (q_tilde_1 + q_tilde_2)\n",
    "        # z_n (B, M, C)\n",
    "\n",
    "        H_T = H.permute(0,2,1).coalesce()\n",
    "\n",
    "        b_cg = 1 / (2 * gamma) * my_bmm(C, (mu_a - mu_b + mu_d - mu_e)) \\\n",
    "            - 1 / gamma * my_bmm(H_T, mu_c) - 1 / 2 * torch.bmm(C, (q_tilde_1 - q_tilde_2)) \\\n",
    "            + my_bmm(H_T, y)\n",
    "        # b_cg (B, N, C)\n",
    "\n",
    "        C_T = C.permute(0,2,1).coalesce()\n",
    "\n",
    "        L = my_bmm(H_T, H) + my_bmm(C,C_T)\n",
    "        # L = SparseCache.eye(B,N)\n",
    "        # L (B, N, N)\n",
    "\n",
    "        g = - b_cg + my_bmm(L, x)\n",
    "        # g (B, N, C)\n",
    "\n",
    "        v = torch.zeros_like(x)\n",
    "        # v (B, N, C)\n",
    "\n",
    "        for t in range(self.cg_steps):\n",
    "            x, v, g = self.cg_update(L, t, x, v, g)\n",
    "\n",
    "        q = torch.zeros_like(q_tilde, device=device)\n",
    "\n",
    "        q[:,:M,:] = 1 / 2 * (z_n - my_bmm(C_T, x)) + 1 / (2 * gamma) * \\\n",
    "              (mu_a - mu_d + gamma * q_tilde_1)\n",
    "        # q_1 (B, M, C)\n",
    "        q[:,M:,:] = 1 / 2 * (z_n + my_bmm(C_T, x)) + 1 / (2 * gamma) * \\\n",
    "              (mu_b - mu_e + gamma * q_tilde_2)\n",
    "        # q_2 (B, M, C)\n",
    "\n",
    "        # q = torch.cat([q_1, q_2], dim=1)\n",
    "        # q (B, 2M, C)\n",
    "\n",
    "        zeros_q = torch.zeros((B,M,channels), device=device)\n",
    "        q_tilde_n = torch.zeros_like(q_tilde, device=device)\n",
    "\n",
    "        q_tilde_1_n = q[:,:M,:] + 1 / (gamma) * mu_d\n",
    "        q_tilde_n[:,:M,:] = torch.maximum(q_tilde_1_n, zeros_q)\n",
    "        q_tilde_2_n = q[:,M:,:] + 1 / (gamma) * mu_e\n",
    "        q_tilde_n[:,M:,:] = torch.maximum(q_tilde_2_n, zeros_q)\n",
    "\n",
    "        # q_tilde_n = torch.cat([q_tilde_1_n, q_tilde_2_n], dim=1)\n",
    "        # q_tilde_n (B, 2M, C)\n",
    "\n",
    "        A = self.build_A(B, K, M, C_T, H, device)\n",
    "\n",
    "        B_mat = self.build_B_mat(B, M, N, A, device)\n",
    "\n",
    "        zxq = torch.cat([z_n, x, q], dim=1)\n",
    "        # zxq (B, M+N+2M, C)\n",
    "\n",
    "        b = self.build_b(B,M,channels,y,device)\n",
    "        # b (B, 2M + K, C)\n",
    "        bq_tilde = torch.cat([b, q_tilde_n.to_sparse_coo()], dim=1)\n",
    "        # bq_tilde (B, 2M + K + 2M, C)\n",
    "\n",
    "        mu_n = mu + gamma * (my_bmm(B_mat,zxq) - bq_tilde)\n",
    "        # mu_n (B, 2M+K+2M, C)\n",
    "        with torch.no_grad():\n",
    "            del B_mat, zxq, b, bq_tilde\n",
    "            # del q_1, q_2,\n",
    "            del q_tilde_1_n, q_tilde_2_n\n",
    "            del A, L, g, v, z_n, b_cg\n",
    "            del mu_a, mu_b, mu_c, mu_d, mu_e\n",
    "            del C_T #, q_tilde_1, q_tilde_2\n",
    "\n",
    "        return x, q_tilde_n, mu_n\n",
    "\n",
    "\n",
    "    def cg_update(self, L, t, x, v, g):\n",
    "        \"\"\"\n",
    "        Conjugate gradient update step.\n",
    "        L: (N, N)\n",
    "        t: int\n",
    "        x: (B, N, C)\n",
    "        v: (B, N, C)\n",
    "        g: (B, N, C)\n",
    "        \"\"\"\n",
    "        a = self.alpha[t]\n",
    "        b = self.beta[t]\n",
    "        g_n = g - a * my_bmm(L, v)\n",
    "        # g_n (B, N, C)\n",
    "        v_n = g_n + b * v\n",
    "        # v_n (B, N, C)\n",
    "        x_n = - a * v_n + x\n",
    "        # x_n (B, N, C)\n",
    "        return x_n, v_n, g_n\n",
    "\n",
    "    def build_A(self,B,K,M,C_T,H,device):\n",
    "        I_S = lambda S: SparseCache.eye(B, S)\n",
    "        I_M = I_S(M) # (M, M)\n",
    "        Z_S = lambda *S: SparseCache.zeros(S)\n",
    "\n",
    "        A_row_0 = torch.cat([I_M, -C_T, -I_M, Z_S(B,M,M)], dim=2)\n",
    "        # (B, M, M+N+M+M)\n",
    "        A_row_1 = torch.cat([I_M, C_T, Z_S(B,M,M), -I_M], dim=2)\n",
    "        # (B, M, M+N+M+M)\n",
    "        A_row_2 = torch.cat([Z_S(B, K, M), H, Z_S(B, K, 2*M)], dim=2)\n",
    "        # (B, K, M+N+M+M)\n",
    "        A = torch.cat([A_row_0, A_row_1, A_row_2], dim=1).detach()\n",
    "        # A = sparse_block_matrix([\n",
    "        #     [I_M, -C_T, -I_M, Z_S(B,M,M)],\n",
    "        #     [I_M, C_T, Z_S(B,M,M), -I_M],\n",
    "        #     [Z_S(B, K, M), H, Z_S(B, K, 2*M)]],1,2)\n",
    "        # A (B, M+M+K, M+N+M+M)\n",
    "        del A_row_0, A_row_1, A_row_2\n",
    "        return A\n",
    "\n",
    "    def build_B_mat(self,B,M,N,A,device):\n",
    "        I_S = lambda S: SparseCache.eye(B, S)\n",
    "        I_M = I_S(M) # (M, M)\n",
    "        Z_S = lambda *S: SparseCache.zeros(S)\n",
    "        B_lower_block = torch.cat([Z_S(B, 2 * M, M + N), I_S(2*M)],dim=2)\n",
    "        # (B, 2M, M+N+M+M)\n",
    "        B_mat = torch.cat([A, B_lower_block], dim=1).detach()\n",
    "        # B_mat = sparse_block_matrix([[A],[Z_S(B, 2 * M, M + N), I_S(2*M)]],1,2)\n",
    "        # B (B, 2M+K+2M, M+N+2M)\n",
    "        del B_lower_block\n",
    "        return B_mat\n",
    "\n",
    "    def build_b(self,B,M,channels,y,device):\n",
    "        Z_S = lambda *S: SparseCache.zeros(S)\n",
    "        b = torch.cat([Z_S(B,M,channels),Z_S(B,M,channels),y.to_sparse_coo()], dim=1)\n",
    "        # b (B, 2M + K, C)\n",
    "        return b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADMMBlock(nn.Module):\n",
    "    def __init__(self, num_admm_layers, cg_steps, edges_p_node, sample_size, device='cuda'):\n",
    "        super(ADMMBlock, self).__init__()\n",
    "\n",
    "        self.num_admm_layers = num_admm_layers\n",
    "        self.cg_steps = cg_steps\n",
    "        self.num_edges = edges_p_node\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "        self.admm_layers = nn.ModuleList([ADMMLayer(cg_steps, edges_p_node, sample_size, device) for _ in range(num_admm_layers)])\n",
    "        self.sample_size = sample_size\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, C, H, y):\n",
    "        \"\"\"\n",
    "        let B = batch_size, N = num_nodes, M = num_edges, K = sample_size, C = num_channels\n",
    "        C: incidence matrix of size (B, N, M)\n",
    "        H: sampling matrix of size (B, K, N)\n",
    "        y: signal to interpolate of size (B, K, C)\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes, num_edges = C.shape\n",
    "        channels = y.shape[-1]\n",
    "        device = self.device\n",
    "\n",
    "        x = torch.sparse_coo_tensor(size=(batch_size, num_nodes, channels), device=device)\n",
    "        # x (B, N, C)\n",
    "        q_tilde = torch.zeros(size=(batch_size, 2 * num_edges, channels), device=device)\n",
    "        # q_tilde (B, 2M, C)\n",
    "        mu = torch.ones(batch_size, 4 * num_edges + self.sample_size, channels, device=device) * 0.1\n",
    "        # mu (B, 4M+K, C)\n",
    "        i = 0\n",
    "        for admm_layer in self.admm_layers:\n",
    "            i += 1\n",
    "            x, q_tilde, mu = admm_layer(C, H, y, x, q_tilde, mu)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "        self.shortcut = nn.Identity() if in_channels == out_channels else \\\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.shortcut.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x) + self.shortcut(x)\n",
    "\n",
    "class ShallowCNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_heads, channels_per_head):\n",
    "        super(ShallowCNN, self).__init__()\n",
    "        self.out_channels = num_heads * channels_per_head\n",
    "        self.channels_per_head  = channels_per_head\n",
    "        self.num_heads = num_heads\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, self.out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            SkipConv(self.out_channels, self.out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            SkipConv(self.out_channels, self.out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            SkipConv(self.out_channels, self.out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        let B = batch_size, N = num_nodes = H * W, M = num_edges, K = sample_size, C = num_channels\n",
    "        x: tensor of values for each graph (B, H, W, C)\n",
    "        \"\"\"\n",
    "        # x: [B, C, H, W]\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        features = self.conv_layers(x)  # [B, 48, H, W]\n",
    "        # Split into 4 heads of 12 channels each\n",
    "        heads = torch.split(features, self.channels_per_head, dim=1)  # Each head: [B, 12, H, W]\n",
    "        heads = [head.permute(0,2,3,1) for head in heads]\n",
    "        return heads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "class GraphLearnBlock(nn.Module):\n",
    "    def __init__(self, in_channels, feature_dim, edges_p_node, num_heads, num_admm_layers, cg_steps, sample_size, device='cuda'):\n",
    "        super(GraphLearnBlock, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.feature_dim = feature_dim\n",
    "        self.edges_p_node = edges_p_node\n",
    "        self.num_heads = num_heads\n",
    "        self.num_admm_layers = num_admm_layers\n",
    "        self.cg_steps = cg_steps\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "        self.shallow_cnn = ShallowCNN(in_channels, num_heads, feature_dim)\n",
    "        self.graph_learnings = nn.ModuleList([GraphLearning(feature_dim, edges_p_node, device) for _ in range(num_heads)])\n",
    "        self.admm_block = ADMMBlock(num_admm_layers, cg_steps, edges_p_node, sample_size, device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, edges, H):\n",
    "        \"\"\"\n",
    "        let B = batch_size, N = num_nodes = H * W, M = num_edges, K = sample_size, C = num_channels\n",
    "        x: predicted tensor of values for each graph (B, H, W, C)\n",
    "        edges: tensor of edge indices for each graph (B, M, 2)\n",
    "        H: sampling matrix of size (B, K, N)\n",
    "        return: new predicted tensor of values for each graph (B, H, W, C)\n",
    "        \"\"\"\n",
    "        batch_size, Ht, Wi, channels = x.shape\n",
    "        features = self.shallow_cnn(x) # features (B, H, W, feature_dim)\n",
    "\n",
    "        x = x.view(batch_size, Ht * Wi, -1)\n",
    "        predicted_xs = []\n",
    "        for i in range(self.num_heads):\n",
    "            feat = features[i].view(batch_size, Ht * Wi, -1)\n",
    "            C = self.graph_learnings[i](feat, edges)\n",
    "            # C sparse(B, N, M)\n",
    "            y = my_bmm(H, x)\n",
    "            # y (B, K, C)\n",
    "            predicted_x = self.admm_block(C, H, y)\n",
    "            # predicted_x (B, N, C)\n",
    "            predicted_xs.append(predicted_x)\n",
    "            del predicted_x, y\n",
    "        x_tensor = torch.stack(predicted_xs, dim=0)\n",
    "        del predicted_xs\n",
    "        # average over stacked dimension\n",
    "        # if x_tensor.is_sparse:\n",
    "        #     x_tensor = x_tensor.coalesce()\n",
    "        #     x_tensor = x_tensor.to_dense()\n",
    "        ret =  torch.mean(x_tensor, dim=0).reshape(batch_size, Ht, Wi, -1) # (B, N, C)\n",
    "        del x_tensor\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class uGLR(nn.Module):\n",
    "    def __init__(self, in_channels, feature_dim, edges_p_node, num_heads, num_admm_layers, cg_steps, sample_size, num_learn_blocks, device='cuda'):\n",
    "        super(uGLR, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.feature_dim = feature_dim\n",
    "        self.edges_p_node = edges_p_node\n",
    "        self.num_heads = num_heads\n",
    "        self.num_admm_layers = num_admm_layers\n",
    "        self.cg_steps = cg_steps\n",
    "        self.sample_size = sample_size\n",
    "        self.num_learn_blocks = num_learn_blocks\n",
    "\n",
    "        self.graph_learn_blocks = nn.ModuleList([GraphLearnBlock(in_channels, feature_dim, edges_p_node, num_heads, num_admm_layers, cg_steps, sample_size, device) for _ in range(num_learn_blocks)])\n",
    "        self.device = device\n",
    "        self.first_admm = ADMMBlock(num_admm_layers, cg_steps, edges_p_node, sample_size, device)\n",
    "\n",
    "    def forward(self, y, edges, H, C, og_size):\n",
    "        \"\"\"\n",
    "        let B = batch_size, N = num_nodes, M = num_edges, K = sample_size, C = num_channels\n",
    "        y: tensor of values for each graph (B, K, C)\n",
    "        edges: tensor of edge indices for each graph (B, M, 2)\n",
    "        H: sampling matrix of size (B, K, N)\n",
    "        C: incidence matrix of size (B, N, M)\n",
    "        return: tensor of interpolated features x (B, N, C)\n",
    "        \"\"\"\n",
    "        B, K, channels = y.shape\n",
    "        x = self.first_admm(C, H, y) # (B, N, C)\n",
    "        Wi, Ht = og_size\n",
    "        x = x.reshape(B, Ht, Wi, -1)\n",
    "        for i in range(self.num_learn_blocks):\n",
    "            x = self.graph_learn_blocks[i](x, edges, H) # (B, N, C)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_batches = 1\n",
    "data_size = batch_size * num_batches\n",
    "img_w = 32\n",
    "img_h = 32\n",
    "channels = 3\n",
    "num_samples = 64\n",
    "neighbor_n = 3\n",
    "\n",
    "feature_dim = 6\n",
    "node_neighbors = neighbor_n * neighbor_n\n",
    "\n",
    "num_heads = 2\n",
    "num_admm_layers = 5\n",
    "num_cg_steps = 10\n",
    "num_gl_blocks = 3\n",
    "\n",
    "E = image_graph_edges(img_w,img_h,neighbor_n)\n",
    "\n",
    "X = torch.randn((data_size, img_w, img_h, channels))\n",
    "\n",
    "H = torch.zeros(img_w, img_h, num_samples)\n",
    "for i in range(num_samples):\n",
    "    x = i % 8\n",
    "    y = i // 8\n",
    "    H[x,y,i] = 1\n",
    "H = H.view(img_w*img_h,num_samples).T\n",
    "batch_H = H.repeat(batch_size,1,1).to_sparse_coo().detach()\n",
    "H = H.repeat(data_size,1,1).to_sparse_coo().detach()\n",
    "\n",
    "X_F = X.view(data_size, img_w * img_h, channels)\n",
    "Y_F = my_bmm(H, X_F)\n",
    "\n",
    "# E = E.repeat(data_size,1,1).detach()\n",
    "batch_E = E.repeat(batch_size,1,1).detach()\n",
    "\n",
    "g = GraphLearning(channels,node_neighbors,'cpu')\n",
    "\n",
    "X_F = X_F\n",
    "batch_E = batch_E\n",
    "\n",
    "batch_C = g(X_F[:batch_size], batch_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "gc.collect()  # Force garbage collection\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "torch.cuda.reset_max_memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = uGLR(channels,num_heads,node_neighbors,num_heads,num_admm_layers,num_cg_steps,num_samples,num_gl_blocks,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_H = batch_H.to(device).detach().requires_grad_(False)\n",
    "batch_E = batch_E.to(device).detach().requires_grad_(False)\n",
    "batch_C = batch_C.to(device).detach().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed X\n",
      "Computed Loss\n",
      "Computing backward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825: UserWarning: Error detected in SliceBackward0. Traceback of forward call that caused the error:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_340975/2921779070.py\", line 12, in <module>\n",
      "    reconstructed_X = model(batch_Y,batch_E,batch_H,batch_C,(img_w,img_h))\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_340975/1526424405.py\", line 32, in forward\n",
      "    x = self.graph_learn_blocks[i](x, edges, H) # (B, N, C)\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_340975/3618521609.py\", line 39, in forward\n",
      "    predicted_x = self.admm_block(C, H, y)\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_340975/2679616332.py\", line 34, in forward\n",
      "    x, q_tilde, mu = admm_layer(C, H, y, x, q_tilde, mu)\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/infres/kbrowder-24/jp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_340975/3425429386.py\", line 150, in forward\n",
      "    mu.narrow(1, 3*M + K, M)\n",
      " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:110.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::as_strided' with arguments from the 'SparseCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:30476 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44679 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:26996 [kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:954 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:462 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:23170 [kernel]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\nNegative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nZeroTensor: registered at aten/src/ATen/RegisterZeroTensor.cpp:164 [kernel]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4942 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:17004 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:321 [backend fallback]\nAutocastXPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:463 [backend fallback]\nAutocastMPS: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:735 [kernel]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1079 [kernel]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[20], line 19\u001b[0m\n",
      "\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing backward\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing backward\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\n",
      "File \u001b[0;32m~/jp/.venv/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n",
      "\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n",
      "\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n",
      "\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n",
      "\u001b[1;32m    580\u001b[0m     )\n",
      "\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n",
      "\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/jp/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n",
      "\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n",
      "\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n",
      "\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n",
      "\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/jp/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n",
      "\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n",
      "\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n",
      "\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n",
      "\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::as_strided' with arguments from the 'SparseCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n",
      "\n",
      "CPU: registered at aten/src/ATen/RegisterCPU.cpp:30476 [kernel]\n",
      "CUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44679 [kernel]\n",
      "Meta: registered at aten/src/ATen/RegisterMeta.cpp:26996 [kernel]\n",
      "QuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:954 [kernel]\n",
      "QuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:462 [kernel]\n",
      "BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\n",
      "Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\n",
      "FuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\n",
      "Functionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:23170 [kernel]\n",
      "Named: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\n",
      "Conjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\n",
      "Negative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\n",
      "ZeroTensor: registered at aten/src/ATen/RegisterZeroTensor.cpp:164 [kernel]\n",
      "ADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4942 [kernel]\n",
      "AutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\n",
      "AutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\n",
      "AutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\n",
      "AutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\n",
      "AutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\n",
      "AutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\n",
      "AutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\n",
      "AutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\n",
      "AutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\n",
      "AutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\n",
      "AutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\n",
      "AutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\n",
      "AutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\n",
      "AutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\n",
      "AutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\n",
      "AutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\n",
      "AutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17993 [autograd kernel]\n",
      "Tracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:17004 [kernel]\n",
      "AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:321 [backend fallback]\n",
      "AutocastXPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:463 [backend fallback]\n",
      "AutocastMPS: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\n",
      "AutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\n",
      "FuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:735 [kernel]\n",
      "BatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\n",
      "FuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\n",
      "Batched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1079 [kernel]\n",
      "VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
      "FuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\n",
      "PythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\n",
      "FuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\n",
      "PreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\n",
      "PythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "for batch_start_index in range(0, data_size, batch_size):\n",
    "    batch_end_index = min(batch_start_index + batch_size, data_size)\n",
    "    batch_X = X[batch_start_index:batch_end_index]\n",
    "    batch_Y = Y_F[batch_start_index:batch_end_index]\n",
    "    batch_X = batch_X.to(device).detach()\n",
    "    batch_Y = batch_Y.to(device).detach()\n",
    "    print(\"Starting model...\")\n",
    "    with profile(activities=[ProfilerActivity.CUDA, ProfilerActivity.CPU], record_shapes=True, profile_memory=True) as prof:\n",
    "        reconstructed_X = model(batch_Y,batch_E,batch_H,batch_C,(img_w,img_h))\n",
    "    print(\"Computed X\")\n",
    "    loss = (reconstructed_X - batch_X).square().mean()\n",
    "    print(\"Computed Loss\")\n",
    "    # make_dot(loss, params=dict(model.named_parameters())).render(\"graph\", format=\"png\")\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Computing backward\")\n",
    "    loss.backward()\n",
    "    print(\"Computing backward\")\n",
    "    optimizer.step()\n",
    "    print(f\"Batch {batch_start_index // batch_size + 1}/{num_batches}, Loss: {loss.detach().item()}\")\n",
    "    del batch_X, batch_Y\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = prof.key_averages(group_by_input_shape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls                                                                      Input Shapes  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                                        aten::_coalesce         0.59%     115.999ms         2.68%     531.246ms     459.953us     180.882ms        21.05%     205.271ms     177.724us           0 b           0 b       2.46 Gb      -2.55 Gb          1155                                                                    [[4096, 4096]]  \n",
      "void at::native::apply::coalesceValuesKernel<float, ...         0.00%       0.000us         0.00%       0.000us       0.000us     100.288ms        11.67%     100.288ms      44.573us           0 b           0 b           0 b           0 b          2250                                                                                []  \n",
      "void at_cuda_detail::cub::DeviceMergeSortMergeKernel...         0.00%       0.000us         0.00%       0.000us       0.000us      86.971ms        10.12%      86.971ms       7.440us           0 b           0 b           0 b           0 b         11690                                                                                []  \n",
      "                                        aten::_coalesce         0.08%      15.467ms         1.83%     362.411ms       3.452ms      82.969ms         9.66%      92.560ms     881.528us           0 b           0 b       2.08 Gb      -2.43 Gb           105                                                                [[409856, 311296]]  \n",
      "void at_cuda_detail::cub::DeviceMergeSortBlockSortKe...         0.00%       0.000us         0.00%       0.000us       0.000us      74.511ms         8.67%      74.511ms      33.116us           0 b           0 b           0 b           0 b          2250                                                                                []  \n",
      "void at_cuda_detail::cub::DeviceMergeSortPartitionKe...         0.00%       0.000us         0.00%       0.000us       0.000us      73.744ms         8.58%      73.744ms       6.308us           0 b           0 b           0 b           0 b         11690                                                                                []  \n",
      "                                        aten::_coalesce         0.18%      35.138ms         0.79%     157.205ms     499.063us      64.424ms         7.50%      74.603ms     236.835us           0 b           0 b       1.18 Gb      -1.51 Gb           315                                                                  [[102400, 4096]]  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      58.365ms         6.79%      58.365ms      63.441us           0 b           0 b           0 b           0 b           920                                                                                []  \n",
      "                            aten::_sparse_sparse_matmul         0.13%      24.911ms         1.00%     197.083ms       1.877ms      46.470ms         5.41%      57.608ms     548.645us           0 b           0 b     525.00 Kb      -5.08 Mb           105                                                        [[4096, 256], [256, 4096]]  \n",
      "                                        aten::_coalesce         0.12%      23.854ms         1.69%     335.123ms       1.596ms      43.909ms         5.11%      50.678ms     241.322us           0 b           0 b     812.31 Mb      -1.02 Gb           210                                                                  [[4096, 102400]]  \n",
      "void cusparse::binary_search_lb_kernel<128u, 8u, 0ul...         0.00%       0.000us         0.00%       0.000us       0.000us      35.295ms         4.11%      35.295ms     164.161us           0 b           0 b           0 b           0 b           215                                                                                []  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      33.626ms         3.91%      33.626ms       3.739us           0 b           0 b           0 b           0 b          8992                                                                                []  \n",
      "                                              aten::cat         1.63%     322.684ms        19.06%        3.772s     541.456us      29.160ms         3.39%      51.730ms       7.426us      31.25 Mb      31.25 Mb      21.61 Gb      12.24 Gb          6966                                                                          [[], []]  \n",
      "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us      22.034ms         2.56%      22.034ms       9.881us           0 b           0 b           0 b           0 b          2230                                                                                []  \n",
      "                                            aten::copy_         0.00%     942.470us         0.16%      31.799ms     302.851us      21.761ms         2.53%      21.761ms     207.252us           0 b           0 b           0 b           0 b           105                                                    [[2, 204800], [2, 204800], []]  \n",
      "void thrust::THRUST_200302_500_600_700_750_800_860_9...         0.00%       0.000us         0.00%       0.000us       0.000us      20.603ms         2.40%      20.603ms       9.157us           0 b           0 b           0 b           0 b          2250                                                                                []  \n",
      "                            aten::_sparse_sparse_matmul         0.11%      22.371ms         1.05%     207.181ms       1.973ms      20.519ms         2.39%      72.443ms     689.934us           0 b           0 b     217.82 Mb      -1.19 Gb           105                                                  [[4096, 102400], [102400, 4096]]  \n",
      "                                            aten::copy_         0.01%       1.868ms         0.20%      38.976ms     185.599us      19.914ms         2.32%      19.914ms      94.829us           0 b           0 b           0 b           0 b           210                                                    [[2, 102400], [2, 102400], []]  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      18.580ms         2.16%      18.580ms       7.873us           0 b           0 b           0 b           0 b          2360                                                                                []  \n",
      "                                            aten::addmm         0.33%      65.884ms         3.74%     740.251ms     665.095us      18.432ms         2.15%     233.499ms     209.793us           0 b           0 b      52.17 Mb      -3.26 Gb          1113                                      [[4096, 3], [4096, 4096], [4096, 3], [], []]  \n",
      "void at_cuda_detail::cub::DeviceSelectSweepKernel<at...         0.00%       0.000us         0.00%       0.000us       0.000us      17.437ms         2.03%      17.437ms       3.873us           0 b           0 b           0 b           0 b          4502                                                                                []  \n",
      "                                          aten::nonzero         0.37%      73.908ms         0.89%     176.752ms      77.796us      17.253ms         2.01%      17.253ms       7.594us           0 b           0 b       1.11 Mb           0 b          2272                                                                             [[3]]  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      16.241ms         1.89%      16.241ms       3.478us           0 b           0 b           0 b           0 b          4670                                                                                []  \n",
      "                                          aten::nonzero         0.24%      48.436ms         0.58%     114.153ms      98.834us      16.010ms         1.86%      16.010ms      13.862us           0 b           0 b     871.33 Mb           0 b          1155                                                                         [[98852]]  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      15.882ms         1.85%      15.882ms       2.818us           0 b           0 b           0 b           0 b          5636                                                                                []  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      15.704ms         1.83%      15.704ms       3.474us           0 b           0 b           0 b           0 b          4520                                                                                []  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us      15.443ms         1.80%      15.443ms       6.329us           0 b           0 b           0 b           0 b          2440                                                                                []  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      15.159ms         1.76%      15.159ms       3.354us           0 b           0 b           0 b           0 b          4520                                                                                []  \n",
      "void convert_CooToCsr_kernel<0>(int const*, int, int...         0.00%       0.000us         0.00%       0.000us       0.000us      14.875ms         1.73%      14.875ms       6.670us           0 b           0 b           0 b           0 b          2230                                                                                []  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      14.851ms         1.73%      14.851ms       3.484us           0 b           0 b           0 b           0 b          4263                                                                                []  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      14.431ms         1.68%      14.431ms       9.784us           0 b           0 b           0 b           0 b          1475                                                                                []  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      13.263ms         1.54%      13.263ms       3.751us           0 b           0 b           0 b           0 b          3536                                                                                []  \n",
      "void cusparse::load_balancing_kernel<256u, 1u, 0ul, ...         0.00%       0.000us         0.00%       0.000us       0.000us      12.168ms         1.42%      12.168ms       6.922us           0 b           0 b           0 b           0 b          1758                                                                                []  \n",
      "                                           aten::arange         0.16%      32.458ms         0.40%      78.500ms      14.695us      11.913ms         1.39%      11.913ms       2.230us      15.63 Mb     800.70 Kb       2.58 Gb           0 b          5342                                                                 [[], [], [], [0]]  \n",
      "void (anonymous namespace)::elementwise_kernel_with_...         0.00%       0.000us         0.00%       0.000us       0.000us      11.913ms         1.39%      11.913ms       2.572us           0 b           0 b           0 b           0 b          4632                                                                                []  \n",
      "                                        aten::_coalesce         0.14%      28.029ms         0.57%     112.865ms     358.303us      11.392ms         1.33%      15.395ms      48.873us           0 b           0 b       1.54 Mb      -1.85 Mb           315                                                                     [[4096, 256]]  \n",
      "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us      11.356ms         1.32%      11.356ms       5.002us           0 b           0 b           0 b           0 b          2270                                                                                []  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.804ms         1.26%      10.804ms       2.400us           0 b           0 b           0 b           0 b          4502                                                                                []  \n",
      "void thrust::THRUST_200302_500_600_700_750_800_860_9...         0.00%       0.000us         0.00%       0.000us       0.000us      10.427ms         1.21%      10.427ms       2.317us           0 b           0 b           0 b           0 b          4500                                                                                []  \n",
      "                                            aten::copy_         0.01%       1.025ms         0.11%      22.204ms     153.128us       9.592ms         1.12%       9.592ms      66.149us           0 b           0 b           0 b           0 b           145                                                          [[204800], [204800], []]  \n",
      "                                              aten::mul         4.27%     845.509ms         4.44%     879.168ms     279.101us       9.574ms         1.11%       9.574ms       3.039us           0 b           0 b     145.69 Mb     145.69 Mb          3150                                                                [[], [1, 4096, 3]]  \n",
      "void cusparse::binary_search_partition_kernel<256, 2...         0.00%       0.000us         0.00%       0.000us       0.000us       9.003ms         1.05%       9.003ms       5.121us           0 b           0 b           0 b           0 b          1758                                                                                []  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.889ms         1.03%       8.889ms       2.167us           0 b           0 b           0 b           0 b          4102                                                                                []  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us       8.718ms         1.01%       8.718ms       1.828us           0 b           0 b           0 b           0 b          4769                                                                                []  \n",
      "                                     aten::index_select         0.07%      13.065ms         0.16%      31.779ms      27.515us       8.013ms         0.93%       8.013ms       6.938us           0 b           0 b       2.57 Gb           0 b          1155                                                         [[3, 98852], [], [98852]]  \n",
      "                                            aten::copy_         0.06%      12.641ms         0.14%      28.298ms      12.250us       7.974ms         0.93%       7.974ms       3.452us           0 b           0 b           0 b           0 b          2310                                                            [[98596], [98596], []]  \n",
      "                                          aten::nonzero         0.11%      21.543ms         0.26%      51.075ms      97.285us       7.764ms         0.90%       7.764ms      14.788us           0 b           0 b     951.01 Mb           0 b           525                                                                        [[200176]]  \n",
      "                                            aten::copy_         0.03%       5.586ms         0.10%      19.613ms      17.669us       7.410ms         0.86%       7.410ms       6.675us           0 b           0 b           0 b           0 b          1110                                                    [[1, 200176], [1, 200176], []]  \n",
      "                                             aten::div_         0.14%      27.780ms         0.21%      41.959ms      18.164us       7.339ms         0.85%       7.339ms       3.177us       1.17 Kb       1.17 Kb           0 b           0 b          2310                                                              [[1, 98596], [], []]  \n",
      "                                             aten::add_         0.06%      10.987ms         0.12%      23.553ms      10.196us       6.948ms         0.81%       6.948ms       3.008us           0 b           0 b           0 b           0 b          2310                                                      [[1, 98596], [1, 98596], []]  \n",
      "                                            aten::addmm         0.06%      11.953ms         0.59%     116.430ms     567.952us       6.926ms         0.81%      19.655ms      95.876us           0 b           0 b       9.61 Mb     -14.31 Mb           205                                        [[4096, 3], [4096, 256], [256, 3], [], []]  \n",
      "void at_cuda_detail::cub::DeviceCompactInitKernel<at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.873ms         0.80%       6.873ms       1.447us           0 b           0 b           0 b           0 b          4749                                                                                []  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us       6.730ms         0.78%       6.730ms       1.846us           0 b           0 b           0 b           0 b          3645                                                                                []  \n",
      "                                            aten::copy_         0.07%      13.131ms         0.17%      33.482ms      14.494us       6.474ms         0.75%       6.474ms       2.803us           0 b           0 b           0 b           0 b          2310                                                      [[1, 98596], [1, 98596], []]  \n",
      "                                     aten::index_select         0.05%      10.380ms         0.13%      24.848ms      21.513us       6.212ms         0.72%       6.212ms       5.379us           0 b           0 b       2.01 Gb           0 b          1155                                                             [[3, 98852], [], [2]]  \n",
      "                                     aten::index_select         0.03%       5.848ms         0.07%      14.074ms      26.808us       6.202ms         0.72%       6.202ms      11.814us           0 b           0 b       2.38 Gb           0 b           525                                                       [[3, 200176], [], [200176]]  \n",
      "                                     aten::index_select         0.01%       1.845ms        20.99%        4.155s      39.569ms       6.167ms         0.72%       6.167ms      58.738us           0 b           0 b       2.46 Gb           0 b           105                                                     [[3, 1015008], [], [1015008]]  \n",
      "void at_cuda_detail::cub::DeviceReduceSingleTileKern...         0.00%       0.000us         0.00%       0.000us       0.000us       5.670ms         0.66%       5.670ms       2.055us           0 b           0 b           0 b           0 b          2759                                                                                []  \n",
      "                                            aten::copy_         0.01%       2.616ms         0.09%      18.276ms      43.515us       5.583ms         0.65%       5.583ms      13.292us           0 b           0 b           0 b           0 b           420                                                          [[102400], [102400], []]  \n",
      "                                            aten::fill_         0.08%      14.908ms         0.17%      33.347ms      11.717us       5.362ms         0.62%       5.362ms       1.884us           0 b           0 b           0 b           0 b          2846                                                                   [[4096, 3], []]  \n",
      "void at_cuda_detail::cub::DeviceReduceKernel<at_cuda...         0.00%       0.000us         0.00%       0.000us       0.000us       5.341ms         0.62%       5.341ms       2.992us           0 b           0 b           0 b           0 b          1785                                                                                []  \n",
      "                                            aten::copy_         0.04%       7.795ms         0.09%      18.521ms      13.568us       5.113ms         0.60%       5.113ms       3.746us           0 b           0 b           0 b           0 b          1365                                                          [[200176], [200176], []]  \n",
      "                                            aten::addmm         0.05%       8.981ms         1.96%     388.602ms       3.701ms       4.983ms         0.58%     101.173ms     963.549us           0 b           0 b     495.53 Mb      -3.59 Gb           105                              [[409856, 3], [409856, 311296], [311296, 3], [], []]  \n",
      "void cusparse::matrix_scalar_multiply_kernel<cuspars...         0.00%       0.000us         0.00%       0.000us       0.000us       4.970ms         0.58%       4.970ms       2.668us           0 b           0 b           0 b           0 b          1863                                                                                []  \n",
      "                                     aten::index_select         0.07%      14.633ms         0.16%      32.287ms      27.954us       4.960ms         0.58%       4.960ms       4.295us           0 b           0 b     435.94 Mb           0 b          1155                                                            [[98852], [], [98852]]  \n",
      "                                              aten::add         3.03%     600.092ms         3.15%     623.532ms     269.927us       4.931ms         0.57%       5.117ms       2.215us           0 b           0 b     108.28 Mb     105.09 Mb          2310                                                  [[1, 4096, 3], [1, 4096, 3], []]  \n",
      "                                            aten::copy_         0.06%      11.005ms         0.10%      19.864ms      13.959us       4.855ms         0.57%       4.855ms       3.412us           0 b           0 b           0 b           0 b          1423                                                        [[3, 4096], [3, 4096], []]  \n",
      "                                            aten::copy_         0.05%       9.693ms         0.09%      17.445ms      12.260us       4.691ms         0.55%       4.691ms       3.296us           0 b           0 b           0 b           0 b          1423                                                        [[4096, 3], [4096, 3], []]  \n",
      "                                        aten::_coalesce         0.06%      11.225ms         0.31%      60.856ms     486.849us       4.608ms         0.54%       6.199ms      49.595us           0 b           0 b     625.00 Kb    -750.00 Kb           125                                                                     [[256, 4096]]  \n",
      "                                              aten::add         1.64%     324.155ms         1.70%     336.081ms     246.213us       4.552ms         0.53%       4.552ms       3.335us           0 b           0 b       1.81 Gb       1.81 Gb          1365                                              [[1, 102400, 3], [1, 102400, 3], []]  \n",
      "                                            aten::addmm         0.06%      12.140ms         0.75%     147.760ms     703.618us       4.453ms         0.52%      58.263ms     277.444us           0 b           0 b     287.15 Mb      -1.44 Gb           210                                  [[102400, 3], [102400, 4096], [4096, 3], [], []]  \n",
      "                                        aten::_coalesce         0.05%       9.545ms         0.10%      20.251ms       1.013ms       4.204ms         0.49%       5.131ms     256.556us           0 b           0 b     109.92 Mb     -96.20 Mb            20                                                               [[1, 4096, 102400]]  \n",
      "                                             aten::div_         0.07%      13.491ms         0.11%      21.775ms      19.617us       4.089ms         0.48%       4.089ms       3.684us         456 b         456 b           0 b           0 b          1110                                                             [[1, 200176], [], []]  \n",
      "void cusparse::binary_search_lb_offset_kernel<128u, ...         0.00%       0.000us         0.00%       0.000us       0.000us       4.040ms         0.47%       4.040ms      18.790us           0 b           0 b           0 b           0 b           215                                                                                []  \n",
      "                                             aten::add_         0.03%       5.345ms         0.06%      11.268ms      10.152us       4.016ms         0.47%       4.016ms       3.618us           0 b           0 b           0 b           0 b          1110                                                    [[1, 200176], [1, 200176], []]  \n",
      "void thrust::THRUST_200302_500_600_700_750_800_860_9...         0.00%       0.000us         0.00%       0.000us       0.000us       3.994ms         0.46%       3.994ms       1.775us           0 b           0 b           0 b           0 b          2250                                                                                []  \n",
      "void at_cuda_detail::cub::DeviceReduceSingleTileKern...         0.00%       0.000us         0.00%       0.000us       0.000us       3.946ms         0.46%       3.946ms       2.088us           0 b           0 b           0 b           0 b          1890                                                                                []  \n",
      "                                              aten::mul         1.10%     217.571ms         1.14%     225.292ms     268.205us       3.843ms         0.45%       3.843ms       4.575us           0 b           0 b       1.09 Gb       1.09 Gb           840                                                             [[1], [1, 102400, 3]]  \n",
      "                                               aten::ne         0.17%      33.647ms         0.24%      46.673ms      20.543us       3.837ms         0.45%       3.837ms       1.689us           0 b           0 b       1.11 Mb       1.11 Mb          2272                                                                         [[3], []]  \n",
      "void cub::CUB_200101_500_520_600_610_700_750_800_860...         0.00%       0.000us         0.00%       0.000us       0.000us       3.485ms         0.41%       3.485ms       3.242us           0 b           0 b           0 b           0 b          1075                                                                                []  \n",
      "                                          aten::nonzero         0.08%      15.124ms         0.18%      36.558ms      83.085us       3.379ms         0.39%       3.379ms       7.679us           0 b           0 b     880.00 Kb           0 b           440                                                                           [[256]]  \n",
      "                                     aten::index_select         0.02%       4.802ms         1.13%     223.402ms     425.528us       3.375ms         0.39%       3.375ms       6.428us           0 b           0 b       1.59 Gb           0 b           525                                                            [[3, 200176], [], [2]]  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.349ms         0.39%       3.349ms       1.969us           0 b           0 b           0 b           0 b          1701                                                                                []  \n",
      "void cusparse::binary_search_partition_kernel<128, 1...         0.00%       0.000us         0.00%       0.000us       0.000us       3.221ms         0.37%       3.221ms       3.745us           0 b           0 b           0 b           0 b           860                                                                                []  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.147ms         0.37%       3.147ms       2.922us           0 b           0 b           0 b           0 b          1077                                                                                []  \n",
      "                                               aten::eq         0.12%      23.824ms         0.19%      37.103ms      32.124us       3.113ms         0.36%       3.113ms       2.695us           0 b           0 b     109.41 Mb     109.41 Mb          1155                                                                     [[98852], []]  \n",
      "void cub::CUB_200101_500_520_600_610_700_750_800_860...         0.00%       0.000us         0.00%       0.000us       0.000us       3.102ms         0.36%       3.102ms      14.426us           0 b           0 b           0 b           0 b           215                                                                                []  \n",
      "                                     aten::index_select         0.03%       6.575ms         0.09%      17.806ms      33.916us       3.049ms         0.35%       3.049ms       5.808us           0 b           0 b     400.93 Mb           0 b           525                                                          [[200176], [], [200176]]  \n",
      "                                            aten::copy_         0.01%       1.138ms         0.02%       3.828ms      18.229us       3.017ms         0.35%       3.017ms      14.368us           0 b           0 b           0 b           0 b           210                                                  [[1, 1015008], [1, 1015008], []]  \n",
      "                                     aten::index_select         0.01%       1.501ms         4.98%     986.388ms       9.394ms       2.958ms         0.34%       2.958ms      28.174us           0 b           0 b       1.62 Gb           0 b           105                                                           [[3, 1015008], [], [2]]  \n",
      "                                              aten::sub         1.67%     330.606ms         1.73%     342.753ms     272.026us       2.918ms         0.34%       2.961ms       2.350us           0 b           0 b      59.06 Mb      58.08 Mb          1260                                                  [[1, 4096, 3], [1, 4096, 3], []]  \n",
      "                                              aten::sub         0.83%     164.768ms         0.87%     171.214ms     232.945us       2.810ms         0.33%       2.810ms       3.823us           0 b           0 b    1014.55 Mb    1014.55 Mb           735                                              [[1, 102400, 3], [1, 102400, 3], []]  \n",
      "void thrust::THRUST_200302_500_600_700_750_800_860_9...         0.00%       0.000us         0.00%       0.000us       0.000us       2.654ms         0.31%       2.654ms      10.329us           0 b           0 b           0 b           0 b           257                                                                                []  \n",
      "                                          aten::nonzero         0.03%       5.505ms         0.07%      13.602ms     129.538us       2.628ms         0.31%       2.628ms      25.032us           0 b           0 b       1.67 Mb           0 b           105                                                                  [[1, 204800, 3]]  \n",
      "                                             aten::add_         0.01%       1.114ms         0.01%       2.244ms      10.687us       2.628ms         0.31%       2.628ms      12.513us           0 b           0 b           0 b           0 b           210                                                  [[1, 1015008], [1, 1015008], []]  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       2.554ms         0.30%       2.554ms       6.081us           0 b           0 b           0 b           0 b           420                                                                                []  \n",
      "                                  aten::scatter_reduce_         0.00%     593.451us         0.01%       1.448ms      72.394us       2.415ms         0.28%       2.523ms     126.151us           0 b           0 b           0 b           0 b            20                                          [[4096], [], [204800], [204800], [], []]  \n",
      "void at::native::_scatter_gather_elementwise_kernel<...         0.00%       0.000us         0.00%       0.000us       0.000us       2.415ms         0.28%       2.415ms     120.748us           0 b           0 b           0 b           0 b            20                                                                                []  \n",
      "                                             aten::div_         0.01%       2.950ms         0.02%       4.330ms      20.617us       2.415ms         0.28%       2.415ms      11.499us          64 b          64 b           0 b           0 b           210                                                            [[1, 1015008], [], []]  \n",
      "                                          aten::nonzero         0.03%       4.980ms         0.06%      11.959ms     113.891us       2.333ms         0.27%       2.333ms      22.220us           0 b           0 b     813.13 Mb           0 b           105                                                                       [[1015008]]  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "Self CPU time total: 19.790s\n",
      "Self CUDA time total: 859.147ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(K.table(sort_by=\"self_cuda_time_total\", row_limit=100))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
