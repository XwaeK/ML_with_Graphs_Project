{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import tracemalloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_graph_edges(H, W, k=5):\n",
    "    \"\"\"\n",
    "    Generates an edges tensor for a graph where each pixel in an HxW image\n",
    "    is connected to its kxk nearest neighbors, shifting the window at borders.\n",
    "\n",
    "    Args:\n",
    "    - H (int): Image height\n",
    "    - W (int): Image width\n",
    "    - k (int): Neighborhood size (kxk)\n",
    "    - device (str): 'cuda' or 'cpu' for tensor allocation\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Edges tensor of shape (num_edges, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    half_k = k // 2\n",
    "\n",
    "    # Create grid of pixel indices\n",
    "    row_indices = torch.arange(H)\n",
    "    col_indices = torch.arange(W)\n",
    "\n",
    "    grid_r, grid_c = torch.meshgrid(row_indices, col_indices, indexing='ij')\n",
    "    pixel_indices = grid_r * W + grid_c  # Convert (row, col) to 1D index\n",
    "\n",
    "    # Create all possible shifts within the kxk neighborhood\n",
    "    d_row = torch.arange(-half_k, half_k + 1)\n",
    "    d_col = torch.arange(-half_k, half_k + 1)\n",
    "\n",
    "    shift_r, shift_c = torch.meshgrid(d_row, d_col, indexing='ij')\n",
    "    shift_r = shift_r.flatten()\n",
    "    shift_c = shift_c.flatten()\n",
    "\n",
    "    # Compute neighbor locations\n",
    "    neighbor_r = grid_r.unsqueeze(-1) + shift_r\n",
    "    neighbor_c = grid_c.unsqueeze(-1) + shift_c\n",
    "\n",
    "    # Shift window for boundary pixels\n",
    "    neighbor_r = neighbor_r.clamp(0, H - 1)\n",
    "    neighbor_c = neighbor_c.clamp(0, W - 1)\n",
    "\n",
    "    # Convert to 1D indices\n",
    "    neighbor_indices = neighbor_r * W + neighbor_c\n",
    "\n",
    "    # Create edges\n",
    "    edges = torch.stack([\n",
    "        pixel_indices.unsqueeze(-1).expand(-1, -1, k * k),  # Expand for each neighbor\n",
    "        neighbor_indices\n",
    "    ], dim=-1)\n",
    "\n",
    "    return edges.reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.sparse\n",
    "import gc\n",
    "\n",
    "class GraphLearning(nn.Module):\n",
    "    def __init__(self, feature_dim, edges_p_node, device='cuda'):\n",
    "        super(GraphLearning, self).__init__()\n",
    "        self.M = nn.Parameter(torch.eye(feature_dim, feature_dim, device=device) * 1.5)\n",
    "        self.edges_p_node = edges_p_node\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, edges):\n",
    "        \"\"\"\n",
    "        let B = batch_size, N = num_nodes, M = num_edges, K = sample_size, C = num_channels, F = feature_dim\n",
    "        x: tensor of features for each graph (B, N, F)\n",
    "        edges: tensor of edge indices for each graph (B, M, 2)\n",
    "        return: incidence matrix C for each graph (B, N, M)\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes, _ = x.shape\n",
    "        num_edges = edges.shape[1]\n",
    "\n",
    "        device = self.device\n",
    "\n",
    "        source_x = x[torch.arange(batch_size).unsqueeze(1), edges[:, :, 0], :] # source_x (B, M, F)\n",
    "        target_x = x[torch.arange(batch_size).unsqueeze(1), edges[:, :, 1], :] # target_x (B, M, F)\n",
    "        d = target_x - source_x # d (B, M, F)\n",
    "        e = torch.einsum(\"bik,kl,bil->bi\", d, self.M, d) # e (B, M)\n",
    "\n",
    "        batch_indices = torch.arange(batch_size).unsqueeze(1).repeat(2, num_edges).view(-1)\n",
    "        # batch_indices (B * 2M, )\n",
    "        row_indices = torch.cat([edges[:, :, 0], edges[:, :, 1]]).view(-1)\n",
    "        # row_indices (B * 2M, )\n",
    "        col_indices = torch.cat([torch.arange(num_edges)] * 2).repeat(batch_size,1).view(-1)\n",
    "        # col_indices (B * 2M, )\n",
    "        idxs = torch.stack([batch_indices.to(device), row_indices, col_indices.to(device)], dim=0)\n",
    "        # idxs (3, B * 2M)\n",
    "\n",
    "        vals = torch.cat([e, -e]).view(-1) # vals (B * 2M, )\n",
    "        vals = softmax(vals, row_indices) # vals (B * 2M, )\n",
    "\n",
    "        C = torch.sparse_coo_tensor(idxs, vals, size=(batch_size, num_nodes, num_edges), device=device, ).coalesce()\n",
    "        # C = C.to_sparse_csr()\n",
    "        with torch.no_grad():\n",
    "            del batch_indices, row_indices, col_indices, idxs, vals, source_x, target_x, d, e\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # C = torch.sparse.softmax(C, dim=2)\n",
    "        C = C.to_sparse_coo()\n",
    "        # C sparse(B, M, N)\n",
    "        return C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.sparse\n",
    "\n",
    "class SparseBMM(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, M1, M2):\n",
    "        assert len(M1.shape) == 3 and len(M2.shape) == 3\n",
    "        B = M1.shape[0]\n",
    "        \n",
    "        # Perform batch sparse-dense matrix multiplication\n",
    "        O = torch.stack([torch.sparse.mm(M1[i], M2[i]) for i in range(B)], dim=0)\n",
    "\n",
    "        # Save tensors for backward\n",
    "        ctx.save_for_backward(M1, M2)\n",
    "        return O\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        M1, M2 = ctx.saved_tensors\n",
    "        B = M1.shape[0]\n",
    "        \n",
    "        # Compute gradients for sparse M1 and dense M2\n",
    "        grad_M1 = torch.stack([torch.sparse.mm(grad_output[i], M2[i].T) for i in range(B)], dim=0)\n",
    "        grad_M2 = torch.stack([torch.sparse.mm(M1[i].T, grad_output[i]) for i in range(B)], dim=0)\n",
    "\n",
    "        return grad_M1, grad_M2\n",
    "\n",
    "# Wrapper function\n",
    "def my_bmm(M1, M2):\n",
    "    return SparseBMM.apply(M1, M2)\n",
    "\n",
    "class SparseCache:\n",
    "    eyes_c = {}\n",
    "    zeros_c = {}\n",
    "    device = 'cpu'\n",
    "    @staticmethod\n",
    "    def set_device(device):\n",
    "        if(SparseCache.device != device):\n",
    "            SparseCache.eyes_c = {}\n",
    "            SparseCache.zeros_C = {}\n",
    "        SparseCache.device = device\n",
    "\n",
    "    @staticmethod\n",
    "    def _sparse_eye(S):\n",
    "        return torch.sparse.spdiags(torch.ones(S),torch.zeros(1, dtype=int),(S,S)).to(SparseCache.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def eye_(S):\n",
    "        SparseCache.eyes_c[(S,S)] = SparseCache.eyes_c.get((S,S),SparseCache._sparse_eye(S))\n",
    "        return SparseCache.eyes_c[(S,S)]\n",
    "\n",
    "    def eye(B,S):\n",
    "        SparseCache.eyes_c[(B,S,S)] = SparseCache.eyes_c.get((B,S,S),torch.stack([SparseCache.eye_(S) for _ in range(B)], dim=0).detach())\n",
    "        return SparseCache.eyes_c[(B,S,S)]\n",
    "\n",
    "    def zeros(S):\n",
    "        SparseCache.zeros_c[S] = SparseCache.zeros_c.get(S,torch.sparse_coo_tensor(size=S, device=SparseCache.device).detach())\n",
    "        return SparseCache.zeros_c[S]\n",
    "\n",
    "def sparse_block_matrix(blocks:list[list[torch.Tensor]],row_dim,col_dim):\n",
    "    nnz = 0\n",
    "    row_dims = []\n",
    "    row_col_dims = []\n",
    "    for row in blocks:\n",
    "        col_dims = []\n",
    "        row_d = row[0].shape[row_dim]\n",
    "        for block in row:\n",
    "            assert block.shape[row_dim] == row_d, \"All blocks in row must have same row dim\"\n",
    "            col_d = block.shape[col_dim]\n",
    "            nnz += block._nnz()\n",
    "            col_dims.append(col_d)\n",
    "        row_dims.append(row_d)\n",
    "        row_col_dims.append(col_dims)\n",
    "        assert sum(col_dims) == sum(row_col_dims[0]), \"Must have consistent number of columns\"\n",
    "    num_dims = len(blocks[0][0].shape)\n",
    "    device = blocks[0][0].device\n",
    "    idxs = torch.empty((num_dims,nnz))\n",
    "    vals = torch.empty((nnz,))\n",
    "    idx_offset = 0\n",
    "    row_offset = 0\n",
    "    for row_i, row in enumerate(blocks):\n",
    "        col_offset = 0\n",
    "        col_dims = row_col_dims[row_i]\n",
    "        for col_i, block in enumerate(row):\n",
    "            block = block.coalesce()\n",
    "            block_idxs = block.indices().clone().detach()\n",
    "            block_vals = block.values()\n",
    "            # check submatrix validity\n",
    "            # _CHECK = torch.sparse_coo_tensor(block_idxs, block_vals, size=block.shape, check_invariants=True).coalesce()\n",
    "            row_idx = block_idxs[row_dim] + row_offset\n",
    "            col_idx = block_idxs[col_dim] + col_offset\n",
    "            block_idxs[row_dim] = row_idx\n",
    "            block_idxs[col_dim] = col_idx\n",
    "            nnz = block._nnz()\n",
    "            idxs[:,idx_offset:idx_offset+nnz] = block_idxs\n",
    "            vals[idx_offset:idx_offset+nnz] = block_vals\n",
    "            del block_idxs, row_idx, col_idx\n",
    "            idx_offset += nnz\n",
    "\n",
    "            col_offset += col_dims[col_i]\n",
    "        row_offset += row_dims[row_i]\n",
    "    out_size = list(blocks[0][0].shape)\n",
    "    out_size[row_dim] = sum(row_dims)\n",
    "    out_size[col_dim] = sum(row_col_dims[0])\n",
    "    out = torch.sparse_coo_tensor(idxs, vals,size=tuple(out_size),device=device,check_invariants=True).coalesce()\n",
    "    del idxs, vals, out_size\n",
    "    return out\n",
    "\n",
    "class ADMMLayer(nn.Module):\n",
    "    def __init__(self, cg_steps, edges_p_node, sample_size, device):\n",
    "        super(ADMMLayer, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.rand(1, device=device))\n",
    "        self.alpha = nn.Parameter(torch.rand(cg_steps, device=device))\n",
    "        self.beta = nn.Parameter(torch.rand(cg_steps, device=device))\n",
    "        self.cg_steps = cg_steps\n",
    "        self.edges_p_node = edges_p_node\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "        self.device = device\n",
    "        SparseCache.set_device(device)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, C, H, y, x, q_tilde, mu):\n",
    "        \"\"\"\n",
    "        let B = batch_size, N = num_nodes, M = num_edges, K = sample_size, C = num_channels\n",
    "        C: incidence matrix of size (B, N, M)\n",
    "        H: sampling matrix of size (B, K, N)\n",
    "        y: signal to interpolate of size (B, K, C)\n",
    "        x: initial guess of size (B, N, C)\n",
    "        q_tilde: initial guess of size (B, 2M, C)\n",
    "        mu: initial guess of size (B, 4M + K, C)\n",
    "        \"\"\"\n",
    "        K = self.sample_size\n",
    "        B, N, channels = x.shape\n",
    "        _, _, M = C.shape\n",
    "        gamma = self.gamma\n",
    "        device = C.device\n",
    "        mu_a, mu_b, mu_c, mu_d, mu_e = mu.narrow(1, 0, M), mu.narrow(1, M, M), \\\n",
    "                                       mu.narrow(1, 2*M, K), mu.narrow(1, 2*M + K, M), \\\n",
    "                                       mu.narrow(1, 3*M + K, M)\n",
    "        # mu_a (B,M,C), mu_b (B,M,C), mu_c (B,K,C), mu_d (B,M,C), mu_e (B,M,C)\n",
    "\n",
    "        q_tilde_1, q_tilde_2 = q_tilde.narrow(1, 0, M), q_tilde.narrow(1, M, M)\n",
    "        # q_tilde_1 (B,M,C), q_tilde_2 (B,M,C)\n",
    "\n",
    "        z_n = -1 / gamma * torch.ones(B,M,channels, device=device) \\\n",
    "              - 1 / (2 * gamma) * (mu_a + mu_b + mu_d + mu_e) \\\n",
    "              + 1 / 2 * (q_tilde_1 + q_tilde_2)\n",
    "        # z_n (B, M, C)\n",
    "\n",
    "        H_T = H.permute(0,2,1)\n",
    "\n",
    "        b_cg = 1 / (2 * gamma) * my_bmm(C, (mu_a - mu_b + mu_d - mu_e)) \\\n",
    "            - 1 / gamma * my_bmm(H_T, mu_c) - 1 / 2 * torch.bmm(C, (q_tilde_1 - q_tilde_2)) \\\n",
    "            + my_bmm(H_T, y)\n",
    "        # b_cg (B, N, C)\n",
    "\n",
    "        C_T = C.permute(0,2,1)\n",
    "\n",
    "        L = my_bmm(H_T, H) + my_bmm(C,C_T)\n",
    "        # L = SparseCache.eye(B,N)\n",
    "        # L (B, N, N)\n",
    "\n",
    "        g = - b_cg + my_bmm(L, x)\n",
    "        # g (B, N, C)\n",
    "\n",
    "        v = torch.zeros_like(x)\n",
    "        # v (B, N, C)\n",
    "\n",
    "        for t in range(self.cg_steps):\n",
    "            x, v, g = self.cg_update(L, t, x, v, g)\n",
    "\n",
    "        q_1 = 1 / 2 * (z_n - my_bmm(C_T, x)) + 1 / (2 * gamma) * \\\n",
    "              (mu_a - mu_d + gamma * q_tilde_1)\n",
    "        # q_1 (B, M, C)\n",
    "        q_2 = 1 / 2 * (z_n + my_bmm(C_T, x)) + 1 / (2 * gamma) * \\\n",
    "              (mu_b - mu_e + gamma * q_tilde_2)\n",
    "        # q_2 (B, M, C)\n",
    "\n",
    "        q = torch.cat([q_1, q_2], dim=1)\n",
    "        # q (B, 2M, C)\n",
    "\n",
    "        q_tilde_1_n = q_1 + 1 / (gamma) * mu_d\n",
    "        q_tilde_1_n = torch.maximum(q_tilde_1_n, torch.zeros_like(q_tilde_1_n, device=device))\n",
    "        q_tilde_2_n = q_2 + 1 / (gamma) * mu_e\n",
    "        q_tilde_2_n = torch.maximum(q_tilde_2_n, torch.zeros_like(q_tilde_2_n, device=device))\n",
    "\n",
    "        q_tilde_n = torch.cat([q_tilde_1_n, q_tilde_2_n], dim=1)\n",
    "        # q_tilde_n (B, 2M, C)\n",
    "\n",
    "        A = self.build_A(B, K, M, C_T, H, device)\n",
    "\n",
    "        B_mat = self.build_B_mat(B, M, N, A, device)\n",
    "\n",
    "        zxq = torch.cat([z_n, x, q], dim=1)\n",
    "        # zxq (B, M+N+2M, C)\n",
    "\n",
    "        b = self.build_b(B,M,channels,y,device)\n",
    "        # b (B, 2M + K, C)\n",
    "        bq_tilde = torch.cat([b, q_tilde_n.to_sparse_coo()], dim=1)\n",
    "        # bq_tilde (B, 2M + K + 2M, C)\n",
    "\n",
    "        mu_n = mu + gamma * (my_bmm(B_mat,zxq) - bq_tilde)\n",
    "        # mu_n (B, 2M+K+2M, C)\n",
    "        with torch.no_grad():\n",
    "            del B_mat, zxq, b, bq_tilde\n",
    "            del q_1, q_2, q_tilde_1_n, q_tilde_2_n\n",
    "            del A, L, g, v, z_n, b_cg\n",
    "            del mu_a, mu_b, mu_c, mu_d, mu_e\n",
    "            del C_T, q_tilde_1, q_tilde_2\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        return x, q_tilde_n, mu_n\n",
    "\n",
    "\n",
    "    def cg_update(self, L, t, x, v, g):\n",
    "        \"\"\"\n",
    "        Conjugate gradient update step.\n",
    "        L: (N, N)\n",
    "        t: int\n",
    "        x: (B, N, C)\n",
    "        v: (B, N, C)\n",
    "        g: (B, N, C)\n",
    "        \"\"\"\n",
    "        a = self.alpha[t]\n",
    "        b = self.beta[t]\n",
    "        g_n = g - a * my_bmm(L, v)\n",
    "        # g_n (B, N, C)\n",
    "        v_n = g_n + b * v\n",
    "        # v_n (B, N, C)\n",
    "        x_n = - a * v_n + x\n",
    "        # x_n (B, N, C)\n",
    "        return x_n, v_n, g_n\n",
    "\n",
    "    def build_A(self,B,K,M,C_T,H,device):\n",
    "        I_S = lambda S: SparseCache.eye(B, S)\n",
    "        I_M = I_S(M) # (M, M)\n",
    "        Z_S = lambda *S: SparseCache.zeros(S)\n",
    "\n",
    "        # A_row_0 = torch.cat([I_M, -C_T, -I_M, Z_S(B,M,M)], dim=2)\n",
    "        # # (B, M, M+N+M+M)\n",
    "        # A_row_1 = torch.cat([I_M, C_T, Z_S(B,M,M), -I_M], dim=2)\n",
    "        # # (B, M, M+N+M+M)\n",
    "        # A_row_2 = torch.cat([Z_S(B, K, M), H, Z_S(B, K, 2*M)], dim=2)\n",
    "        # # (B, K, M+N+M+M)\n",
    "        # A = torch.cat([A_row_0, A_row_1, A_row_2], dim=1).detach()\n",
    "        # # A (B, M+M+K, M+N+M+M)\n",
    "        # del A_row_0, A_row_1, A_row_2\n",
    "        A = sparse_block_matrix([\n",
    "            [I_M, -C_T.coalesce(), -I_M, Z_S(B,M,M)],\n",
    "            [I_M, C_T.coalesce(), Z_S(B,M,M), -I_M],\n",
    "            [Z_S(B, K, M), H.coalesce(), Z_S(B, K, 2*M)]],1,2).detach()\n",
    "        return A\n",
    "\n",
    "    def build_B_mat(self,B,M,N,A,device):\n",
    "        I_S = lambda S: SparseCache.eye(B, S)\n",
    "        I_M = I_S(M) # (M, M)\n",
    "        Z_S = lambda *S: SparseCache.zeros(S)\n",
    "        # B_lower_block = torch.cat([Z_S(B, 2 * M, M + N), I_S(2*M)],dim=2)\n",
    "        # # (B, 2M, M+N+M+M)\n",
    "        # B_mat = torch.cat([A, B_lower_block], dim=1).detach()\n",
    "        # # B (B, 2M+K+2M, M+N+2M)\n",
    "        # del B_lower_block\n",
    "        B_mat = sparse_block_matrix([[A.coalesce()],[Z_S(B, 2 * M, M + N), I_S(2*M)]],1,2).detach()\n",
    "        return B_mat\n",
    "\n",
    "    def build_b(self,B,M,channels,y,device):\n",
    "        Z_S = lambda *S: SparseCache.zeros(S)\n",
    "        b = torch.cat([Z_S(B,M,channels),Z_S(B,M,channels),y.to_sparse_coo()], dim=1)\n",
    "        # b (B, 2M + K, C)\n",
    "        return b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADMMBlock(nn.Module):\n",
    "    def __init__(self, num_admm_layers, cg_steps, edges_p_node, sample_size, device='cuda'):\n",
    "        super(ADMMBlock, self).__init__()\n",
    "\n",
    "        self.num_admm_layers = num_admm_layers\n",
    "        self.cg_steps = cg_steps\n",
    "        self.num_edges = edges_p_node\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "        self.admm_layers = nn.ModuleList([ADMMLayer(cg_steps, edges_p_node, sample_size, device) for _ in range(num_admm_layers)])\n",
    "        self.sample_size = sample_size\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, C, H, y):\n",
    "        \"\"\"\n",
    "        let B = batch_size, N = num_nodes, M = num_edges, K = sample_size, C = num_channels\n",
    "        C: incidence matrix of size (B, N, M)\n",
    "        H: sampling matrix of size (B, K, N)\n",
    "        y: signal to interpolate of size (B, K, C)\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes, num_edges = C.shape\n",
    "        channels = y.shape[-1]\n",
    "        device = self.device\n",
    "\n",
    "        x = torch.sparse_coo_tensor(size=(batch_size, num_nodes, channels), device=device)\n",
    "        # x (B, N, C)\n",
    "        q_tilde = torch.zeros(size=(batch_size, 2 * num_edges, channels), device=device)\n",
    "        # q_tilde (B, 2M, C)\n",
    "        mu = torch.ones(batch_size, 4 * num_edges + self.sample_size, channels, device=device) * 0.1\n",
    "        # mu (B, 4M+K, C)\n",
    "        i = 0\n",
    "        for admm_layer in self.admm_layers:\n",
    "            i += 1\n",
    "            x, q_tilde, mu = admm_layer(C, H, y, x, q_tilde, mu)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "        self.shortcut = nn.Identity() if in_channels == out_channels else \\\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.shortcut.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x) + self.shortcut(x)\n",
    "\n",
    "class ShallowCNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_heads, channels_per_head):\n",
    "        super(ShallowCNN, self).__init__()\n",
    "        self.out_channels = num_heads * channels_per_head\n",
    "        self.channels_per_head  = channels_per_head\n",
    "        self.num_heads = num_heads\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, self.out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            SkipConv(self.out_channels, self.out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            SkipConv(self.out_channels, self.out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            SkipConv(self.out_channels, self.out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        let B = batch_size, N = num_nodes = H * W, M = num_edges, K = sample_size, C = num_channels\n",
    "        x: tensor of values for each graph (B, H, W, C)\n",
    "        \"\"\"\n",
    "        # x: [B, C, H, W]\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        features = self.conv_layers(x)  # [B, 48, H, W]\n",
    "        # Split into 4 heads of 12 channels each\n",
    "        heads = torch.split(features, self.channels_per_head, dim=1)  # Each head: [B, 12, H, W]\n",
    "        heads = [head.permute(0,2,3,1) for head in heads]\n",
    "        return heads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "class GraphLearnBlock(nn.Module):\n",
    "    def __init__(self, in_channels, feature_dim, edges_p_node, num_heads, num_admm_layers, cg_steps, sample_size, device='cuda'):\n",
    "        super(GraphLearnBlock, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.feature_dim = feature_dim\n",
    "        self.edges_p_node = edges_p_node\n",
    "        self.num_heads = num_heads\n",
    "        self.num_admm_layers = num_admm_layers\n",
    "        self.cg_steps = cg_steps\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "        self.shallow_cnn = ShallowCNN(in_channels, num_heads, feature_dim)\n",
    "        self.graph_learnings = nn.ModuleList([GraphLearning(feature_dim, edges_p_node, device) for _ in range(num_heads)])\n",
    "        self.admm_block = ADMMBlock(num_admm_layers, cg_steps, edges_p_node, sample_size, device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, edges, H):\n",
    "        \"\"\"\n",
    "        let B = batch_size, N = num_nodes = H * W, M = num_edges, K = sample_size, C = num_channels\n",
    "        x: predicted tensor of values for each graph (B, H, W, C)\n",
    "        edges: tensor of edge indices for each graph (B, M, 2)\n",
    "        H: sampling matrix of size (B, K, N)\n",
    "        return: new predicted tensor of values for each graph (B, H, W, C)\n",
    "        \"\"\"\n",
    "        batch_size, Ht, Wi, channels = x.shape\n",
    "        features = self.shallow_cnn(x) # features (B, H, W, feature_dim)\n",
    "\n",
    "        x = x.view(batch_size, Ht * Wi, -1)\n",
    "        predicted_xs = []\n",
    "        for i in range(self.num_heads):\n",
    "            feat = features[i].view(batch_size, Ht * Wi, -1)\n",
    "            C = self.graph_learnings[i](feat, edges)\n",
    "            # C sparse(B, N, M)\n",
    "            y = my_bmm(H, x)\n",
    "            # y (B, K, C)\n",
    "            predicted_x = self.admm_block(C, H, y)\n",
    "            # predicted_x (B, N, C)\n",
    "            predicted_xs.append(predicted_x)\n",
    "            del predicted_x, y\n",
    "        x_tensor = torch.stack(predicted_xs, dim=0)\n",
    "        del predicted_xs\n",
    "        # average over stacked dimension\n",
    "        # if x_tensor.is_sparse:\n",
    "        #     x_tensor = x_tensor.coalesce()\n",
    "        #     x_tensor = x_tensor.to_dense()\n",
    "        ret =  torch.mean(x_tensor, dim=0).reshape(batch_size, Ht, Wi, -1) # (B, N, C)\n",
    "        del x_tensor\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class uGTV(nn.Module):\n",
    "    def __init__(self, in_channels, feature_dim, edges_p_node, num_heads, num_admm_layers, cg_steps, sample_size, num_learn_blocks, device='cuda'):\n",
    "        super(uGTV, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.feature_dim = feature_dim\n",
    "        self.edges_p_node = edges_p_node\n",
    "        self.num_heads = num_heads\n",
    "        self.num_admm_layers = num_admm_layers\n",
    "        self.cg_steps = cg_steps\n",
    "        self.sample_size = sample_size\n",
    "        self.num_learn_blocks = num_learn_blocks\n",
    "\n",
    "        self.graph_learn_blocks = nn.ModuleList([GraphLearnBlock(in_channels, feature_dim, edges_p_node, num_heads, num_admm_layers, cg_steps, sample_size, device) for _ in range(num_learn_blocks)])\n",
    "        self.device = device\n",
    "        self.first_admm = ADMMBlock(num_admm_layers, cg_steps, edges_p_node, sample_size, device)\n",
    "\n",
    "    def forward(self, y, edges, H, C, og_size):\n",
    "        \"\"\"\n",
    "        let B = batch_size, N = num_nodes, M = num_edges, K = sample_size, C = num_channels\n",
    "        y: tensor of values for each graph (B, K, C)\n",
    "        edges: tensor of edge indices for each graph (B, M, 2)\n",
    "        H: sampling matrix of size (B, K, N)\n",
    "        C: incidence matrix of size (B, N, M)\n",
    "        return: tensor of interpolated features x (B, N, C)\n",
    "        \"\"\"\n",
    "        B, K, channels = y.shape\n",
    "        x = self.first_admm(C, H, y) # (B, N, C)\n",
    "        Wi, Ht = og_size\n",
    "        x = x.reshape(B, Ht, Wi, -1)\n",
    "        for i in range(self.num_learn_blocks):\n",
    "            x = self.graph_learn_blocks[i](x, edges, H) # (B, N, C)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_batches = 5\n",
    "data_size = batch_size * num_batches\n",
    "img_w = 32\n",
    "img_h = 32\n",
    "channels = 3\n",
    "num_samples = 256\n",
    "neighbor_n = 3\n",
    "\n",
    "feature_dim = 3\n",
    "node_neighbors = neighbor_n * neighbor_n\n",
    "\n",
    "num_heads = 4\n",
    "num_admm_layers = 5\n",
    "num_cg_steps = 10\n",
    "num_gl_blocks = 5\n",
    "\n",
    "E = image_graph_edges(img_w,img_h,neighbor_n)\n",
    "\n",
    "X = torch.randn((data_size, img_w, img_h, channels))\n",
    "\n",
    "H = torch.zeros(img_w, img_h, num_samples)\n",
    "for i in range(num_samples):\n",
    "    x = i % 16\n",
    "    y = i // 16\n",
    "    H[x,y,i] = 1\n",
    "H = H.view(img_w*img_h,num_samples).T\n",
    "batch_H = H.repeat(batch_size,1,1).to_sparse_coo().detach()\n",
    "H = H.repeat(data_size,1,1).to_sparse_coo().detach()\n",
    "\n",
    "X_F = X.view(data_size, img_w * img_h, channels)\n",
    "Y_F = my_bmm(H, X_F)\n",
    "\n",
    "# E = E.repeat(data_size,1,1).detach()\n",
    "batch_E = E.repeat(batch_size,1,1).detach()\n",
    "\n",
    "g = GraphLearning(channels,node_neighbors,'cpu')\n",
    "\n",
    "X_F = X_F\n",
    "batch_E = batch_E\n",
    "\n",
    "batch_C = g(X_F[:batch_size], batch_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "gc.collect()  # Force garbage collection\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.reset_max_memory_allocated()\n",
    "# torch.cuda.reset_max_memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = uGTV(channels,num_heads,node_neighbors,num_heads,num_admm_layers,num_cg_steps,num_samples,num_gl_blocks,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_H = batch_H.to(device).detach().requires_grad_(False)\n",
    "batch_E = batch_E.to(device).detach().requires_grad_(False)\n",
    "batch_C = batch_C.to(device).detach().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot get indices on an uncoalesced tensor, please call .coalesce() first",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m batch_X \u001b[38;5;241m=\u001b[39m batch_X\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto_sparse_coo()\n\u001b[1;32m      8\u001b[0m batch_Y \u001b[38;5;241m=\u001b[39m batch_Y\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto_sparse_coo()\n\u001b[0;32m----> 9\u001b[0m reconstructed_X \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_E\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_H\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_C\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputed X\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m (reconstructed_X \u001b[38;5;241m-\u001b[39m batch_X)\u001b[38;5;241m.\u001b[39msquare()\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/jp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[65], line 28\u001b[0m, in \u001b[0;36muGTV.forward\u001b[0;34m(self, y, edges, H, C, og_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03mlet B = batch_size, N = num_nodes, M = num_edges, K = sample_size, C = num_channels\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03my: tensor of values for each graph (B, K, C)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mreturn: tensor of interpolated features x (B, N, C)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m B, K, channels \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst_admm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, N, C)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m Wi, Ht \u001b[38;5;241m=\u001b[39m og_size\n\u001b[1;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(B, Ht, Wi, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/jp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[62], line 34\u001b[0m, in \u001b[0;36mADMMBlock.forward\u001b[0;34m(self, C, H, y)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m admm_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madmm_layers:\n\u001b[1;32m     33\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 34\u001b[0m     x, q_tilde, mu \u001b[38;5;241m=\u001b[39m \u001b[43madmm_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_tilde\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/jp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[61], line 190\u001b[0m, in \u001b[0;36mADMMLayer.forward\u001b[0;34m(self, C, H, y, x, q_tilde, mu)\u001b[0m\n\u001b[1;32m    187\u001b[0m q_tilde_n \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([q_tilde_1_n, q_tilde_2_n], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# q_tilde_n (B, 2M, C)\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_A\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC_T\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m B_mat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_B_mat(B, M, N, A, device)\n\u001b[1;32m    194\u001b[0m zxq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([z_n, x, q], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[61], line 250\u001b[0m, in \u001b[0;36mADMMLayer.build_A\u001b[0;34m(self, B, K, M, C_T, H, device)\u001b[0m\n\u001b[1;32m    239\u001b[0m Z_S \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39mS: SparseCache\u001b[38;5;241m.\u001b[39mzeros(S)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# A_row_0 = torch.cat([I_M, -C_T, -I_M, Z_S(B,M,M)], dim=2)\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# # (B, M, M+N+M+M)\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# A_row_1 = torch.cat([I_M, C_T, Z_S(B,M,M), -I_M], dim=2)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# # A (B, M+M+K, M+N+M+M)\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# del A_row_0, A_row_1, A_row_2\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[43msparse_block_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mI_M\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mC_T\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mI_M\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ_S\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mI_M\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC_T\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ_S\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mI_M\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mZ_S\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ_S\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m A\n",
      "Cell \u001b[0;32mIn[61], line 84\u001b[0m, in \u001b[0;36msparse_block_matrix\u001b[0;34m(blocks, row_dim, col_dim)\u001b[0m\n\u001b[1;32m     82\u001b[0m col_dims \u001b[38;5;241m=\u001b[39m row_col_dims[row_i]\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col_i, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(row):\n\u001b[0;32m---> 84\u001b[0m     block_idxs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     85\u001b[0m     block_vals \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# check submatrix validity\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# _CHECK = torch.sparse_coo_tensor(block_idxs, block_vals, size=block.shape, check_invariants=True).coalesce()\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot get indices on an uncoalesced tensor, please call .coalesce() first"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "for batch_start_index in range(0, data_size, batch_size):\n",
    "    batch_end_index = min(batch_start_index + batch_size, data_size)\n",
    "    batch_X = X[batch_start_index:batch_end_index]\n",
    "    batch_Y = Y_F[batch_start_index:batch_end_index]\n",
    "    batch_X = batch_X.to(device).detach().to_sparse_coo()\n",
    "    batch_Y = batch_Y.to(device).detach().to_sparse_coo()\n",
    "    reconstructed_X = model(batch_Y,batch_E,batch_H,batch_C,(32,32))\n",
    "    print(\"Computed X\")\n",
    "    loss = (reconstructed_X - batch_X).square().mean()\n",
    "    print(\"Computed Loss\")\n",
    "    # make_dot(loss, params=dict(model.named_parameters())).render(\"graph\", format=\"png\")\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Computing backward\")\n",
    "    loss.backward()\n",
    "    print(\"Computing backward\")\n",
    "    optimizer.step()\n",
    "    print(f\"Batch {batch_start_index // batch_size + 1}/{num_batches}, Loss: {loss.detach().item()}\")\n",
    "    del batch_X, batch_Y\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_x = torch.randn(1,32*32,3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_y = my_bmm(batch_H, og_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_x = model(my_y,batch_E,batch_H,batch_C,(32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "np_x = og_x[0].view(32,32,-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwAlke5sphe20huGLTGMvnblgcs2FZjhjtUdio6ZarCed5rRWCstoiG3aWOJlkiO1pSzKmDk7FO445c8clRQ1JXgkdL2IhkiSSKRWCthlywVRtOMJIpbIY7Rg5zuuxXLxWSQsL2XzlEqxySyAqowpYsmAqhRuZiOWZcgYrKqrQSirve2+mm3f1+zqae7Tjywtfp8+z6PWzet7pMgE11banHIkbK7RMscsoYRvxwdvlc7wOQpJOCPlIUF10jvpcsib2dpWYq9y3mWzlwAF2DBDKWypZsAg9zm0L6MLDZ20glIjMLxrIfOBaVCqs+QVGX6n5eAMrVCQDYtxLt+3syxCW1UCIGQb1cNg5Uu5JwcEMRjbnO0FUtFuHuqy6O/+Sf43XmiG1ZQV+aN7b26dNb39Or7smZY5TsWK5NpG4iaOSZpHijIDnJjGeMqfv4+XP8AEMKsFxLL9uba08aNEltIN8r5jUZkUMu8Fl6AAHC4J+bDpLTzNTuLua3aebzMl0mZh87Y2jqEVV5J29eQPuhq902qxXED/wBmwebFbyNCYoXQOTkGMh2AywG4Agtjt1xMnzaU7dvn5dLtJXenqYKKqe4na19+unnvq3rpdW00V2+b/Z99GbeQwXBUq4kdCUHy72ZwPLJ3qhYEcArgckroJDdahbJHsc3Qug6SzYUKjMQABkqAdoYHJ3ZwpJJqlZ3sYuZoraKKJ0PmJBduFzg7yA2egT5jkMQzHjApLLT5pLBFkluIYLePY8G0wl8Hc+7liy7T1BGdyrwRzMociT6/jrvtr5dnq9bO+/MpfFGz22ve2is99euyv5n/2Q==",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "og = to_pil_image(og_x[0].view(32,32,3).permute(2,0,1))\n",
    "og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAQABADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwAkltrmS3vROzXBdjGZ5GO3puY5BAJZNoXJHoMbjUC6k8Ekd6kkKMMrJFLErDcBhVDHaMZVslWGWDDGeWv7JPtIsImjS0Vkjka2ZQ0UgbYGYylcnIychj8q9flIbcxW0Um6GNJ/NuneNJV2qpD+WxYgYVQApLNljwcDFZKpBWilq9r7a9NuvX+Xuae7TjyR3ttv6rrqurV73ukf/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAIAAACQkWg2AAADG0lEQVR4AQEQA+/8Ad/OxokexbYF4DH00e0jajxIlS8QDrlQgdQBg4PF2foGtcbhi+LZmOtrlUH8zvwr+wBb3BuSAzltFAVZ2a0aHLu8Sgb0JCW+1dRqqJD00D4yP2rTnwDgv0THz/dquq7K02QE5baAAMzUSus1sG+2GHT2REHi4J2plnO/a/scUmxcywCQbbRmI9ei3ezqCIq9Kh9qBDX03ZUocyO4sSGU95SZQP/Kdk8DSKgsOvcUojbqwp/hSpVxNROES6HttxmUhB2UFwHMjsR3DjugiHegg0DgHVxSfH2rC7z7LwC/1IM37PpHufIUO2TFMfO8S39s6WcyzeYAYz+e/E89tZ8p8cs29PIMTSzH1OEWdZrUYiI1Pr4q2C99rtLl5zW+sn+ZDUmIRVkGAdlFHT8MOlBYoTvVV6SMK1F5iDX1/CPLU3VJahTBArfu4s/e3TObIH1OfLPMpBfMTAJQLoGktcwr/9Bw32ryygnwx3q6ksaHSi9K4Ku8v6witnkHQlw6+5RxfRNMOWnopRYAq09wEZyYpw8e5eQqWKzw59gHCVHyV9tj8HsRYFQMShpb4NRewjQqeFaTEFCtFbQdATshOPq3msIZQ/cmnc27OQFapwsVaB8em8IfRPQ6oj2J+Q4P7G47ec/hRUtbKw7oPQAx5GPYyPb/Ldm1fwIsw/Lbmj9RVqQHKm6C1gz9C1tCFHIit8Tr1y8cxMV9m2KC8TsBmH5bsVs0HL7nNYKYAAufGAGMErPPVyIP4u9qlTDSR48i9QE6wJbQLvpYokq+cOZMAflEJ8fXSI109GP2Gf2wpRV86eckJPQdScH1+mjc58kVlvRGqzZdrDdP+SjPPAaF+QDN0Zvc7ig+2duNZpxHHycvKflgDgF12QaxztojS9L6cLbcuAvj6Q05mgYZB7Z+7qcEnB2JIEcLv4jAh9igj7Vr69kGFMJoBCR7NuyG+jG8+mnwhsRT+ilVkitT3iHXMevvAObITNAI6yBUzC6ZQs54yy7hL/YLlG+1NIvJPyJRaSP4Gyft3Up7Bp2tIuVjqE5W7St+gxayYySsAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=16x16>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oy = to_pil_image(my_y[0].view(16,16,3).permute(2,0,1))\n",
    "oy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwCNDp0hkkt9SFiP3aySSssiqwPyqy8lnwj5/hwDglWwZI7i6V7tSim4jmjXzmjJVmQIn3iqoxDfKW4I5ADZFRA2+oWyvCvnzxyYzb258lQ2ExIGDFvmOcdCzEfMBxdtbicwwSKlzNeuoGJQxUlgcoxIIkViUH3ixbaThRxCnJNparq307637LRbLbrqr05tTgvJJrfqtLa9L20S23PHvHl3eXPiSeO8+UwSzxrF8pMX+kSllLDhvmLc+9cxW54vLnxJOz3DXHmRxSrI5BYh41f5sADd83zf7WeSeTh1bd22XOUZO8FZWX5HXf8ACe3a6lLJFa2qWL7R5H2Cz34UNt+b7PtyCzchAcHFR3/jrULiVGtI4oMJtZpoIZpGOQc7vLGOVXgDtXK0VDgm03rbzZnyq7ffQfLLJPM800jSSyMWd3OWYnkkk9TTKKKspK2iP//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAEpklEQVR4Ae1VW1ATZxT+d5ckhIREGspF7iEWQQQs2AbopMOtFi0UUAsIxYpggYiDVqhV1GIROwWGm2AdpiBV7hCoXK1gQJlBQBEtlHu5RGEgCDRIyIXsdkM7Y2f6sr71gfOy/+ye/5zvfN/Zc4Dn1QLNus6YnQfduJXFwxW6zdu+eEY3KbwUk4POlchNPI1jT5d1tHbuvrGLmXozVSBpefHxFsriOkaCAAYIGDz2pCnmfl8vtzyHNnY+iJlh/qWhJ+2HQNeIaZfi/b+zAw6ZYV35d3fwTX/O1Mo3cCpyoI/JVBQYYITC4wgiI9cW245tz7OpJAcc+Ipc7BbjGNjt4hr7rv5AVG7wEHnYTx4HzRRO5EYOOFrb/nQfrGtBsBIDMAH0ahc4xfE3amlDvL3PeAc9D+zFzlYYdzt92McxfOhW58e4QpY2NdDmxzSrOrgzcvKTVB6Kg0cRgvyoEwg8c4XDHi7p8pcOJHHfj+/dOtpYrJpbFPaEpCOZnFLvG8KH1CMR/mUW7vKz9QU053WyXANAGDEB1CXYcGjpa5ONKaJvPOh3rDxQFKmZ3p68qihMS7s7k7Tf4aldUl+1KHo0W3wc0W8/XDcCoVQAoeqrhAy2LJCGGATyFuu7DE+ZP229xm9gRYjH7518FU4ZNtlTLbojFlwuiV5it5hVzZm0+Q4pVXKiDfQ3gHn7YK5tgLcF+9zWFNV34yx2RL4nVc9rd7L0o1+opJwHpuWPf02KmqqY3eXl3JhVCR5LMDoMVITQbzjJvUZ6teITyk8wEtJF+uc0r5V9Gl90UcduSPm5c/+wTp2soPMiJ8xvQkodq1Xs4E8tqHQp0DoKIKIprse1TkY2JeVMJgyEGJfaGLmOyCyLpCNMDYf+tiDzvkET3nGkPdNjAHxCX3s0Nb+GUgxgVIZ3B8EE0DtT/E6EV+/M9G2PDgjnrsxI+1uXQk9ZJjMHB+mHowdmsp1mLY629XxwRJzswFYofc7wgWQCAE0AiClN2Sm9EBq25SBZkB1bI+sV1N6+nmfxzJrlm8ioczFc6vo+Nf2Yn+Tr2WZhQoZdvetlTTENhZQQYYpgtK3boPgAJ7eI4t2aujqZZXGelhpgd3pcKijZm2g13zNVslq1UnPxxKykQM/Y9E9dWFuJwSRA/D8AHRYd7VetB2pM6yM7ztgG7rvimaEfP4IYeTkvmYmFicmjz4OFNcvuOszaRvn7n8XRgALFIMISA5g1zokuakSywt1Gdfk3I7TDqse6BnlR9i5W9hyhza1Ht918ni8fWk4L+xbNnrfnd8lZUgiDMeIZdIImeHr2F0K9m/22+g6dXEYbRCyetPplvXu71YNBl9JZf7RFY9S/PzPp3h7uqmIfBMsAihBsIdwNMjLXI9Epy9N/SFG2amHp7W3aDGRyjoEsAIVEriKjjAURicNUyJCltwD0YkWTsQwDhEJcA2jtlQxCNBAYQygIplKqMBIqhxAMIeMTGQIaGqgKn2wwrFTBMAxhCAYjKPoGCgM8OD5/8fWBG87rhnyQ+ojvE/VK2Xi8Jvyft8QZwgcvttHSr2P8p/p/f3qDyJuumwxsMrDJwP+Igb8Ad5T/7zFeut8AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec = to_pil_image(rec_x[0].view(32,32,3).permute(2,0,1))\n",
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwAlke5sphe20huGLTGMvnblgcs2FZjhjtUdio6ZarCed5rRWCstoiG3aWOJlkiO1pSzKmDk7FO445c8clRQ1JXgkdL2IhkiSSKRWCthlywVRtOMJIpbIY7Rg5zuuxXLxWSQsL2XzlEqxySyAqowpYsmAqhRuZiOWZcgYrKqrQSirve2+mm3f1+zqae7Tjywtfp8+z6PWzet7pM+eaKKK1MwooooAKKKKAP/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAADfUlEQVR4AWO4f+5Y9B1ph0mzS9sqzvQdSbafd9NVVrN43mr/AmtFC8MnyTPqon+6qJ+9ODvzncqzEz4MpIKMN92TmC0nnc9X//7Iec7/P/62EoHhe9iUBefMML1x6caJb543+w9I5995p9Fpqn+B4zWp5rNsY33AKnjG67Wp8o6NM+c+3ln9JoBr9/XwC9PSwwIefecIPSKT9by35MvH15v3dxxQnv6QYQ1pVjAZfrn45uiKDX1fFUO/L2h2sNHhtb4a3rD1o91Vnu9qc7cFaZo2zUj+Junfe7C9cQGfaxlpxgNV23Ask5LZbTQh8ljpqeQjQl8+8bhzVVleaY5Y82H3pdc6hz/NUlm71lTRXV792pVJ5ypOk2xBRYD1Hi82hosvjs6O2+rQ76tzfEYzU4dXzfMb7Ht0Jt2eZb9J1fLQRi59zZ8H1/7Xe6hPqgVM+gJ8FrfkH8xdmXXNYzX3nvat/qZf//w/4KFU+ZpbNKNsT6rQ5vPPVVQMn3KIHMo6oZVKsgU7AxpPbVxfcotpBbPVb32GhTv3KZ8O/tC8x6+rUF5u9rYrp8KV+L/IeoqeZmXpq65ctYBkC64wNq+5vCf7t+R3kUX7rzS/7Ugs9cyyuP9s5oJ1h+RdqtfMe/Q+6+DXXzZfr5i9aZMR4SbZguajN7s01rXnvFsneMj8za87c76KHGRqW8BccPP3F6tF1ab+Uw0uZdx5XlT749fkPdM7tEi1gOUX21a7/P7TDBPmP/SSufzJwSY4gueR0tP1rzT9bd/+dOUUn97PvE208rrqiV+ZH/SFNzFwkGYF07GH3QsTpuWK2CRPMBWxTrm2OOP8vbtbsg5M28XMx//mweKgr4xWX9xWP/IIzTviwfL1EGnGMzAwPbo5g1fBRfn6IuEWjqOGny2TbxrPVkgLiXiUcCbPuvKkQvaBaRfMYtewGzL9ygxV7tMk2YLX2VOfC2y+++bmwvPP93jXn/a6XetXs+XC721Kmecfuhq+nab3K8Lc/2fYxp/52sHBRjIkW+D459zi19s5urokDcNzXqZHn3q/+cySvo5fM35JeUdrJ16fu8hrn8Z5mwe5G+69bvfY1kayBX+0fydIbmuyTHZlEDc6+8xCoE78jM9mi5+sKQV8L2xZw24WPPNha/2Z+vyj4cb3mZ9dSbVgVP1oCIyGwGgIjIbAaAiAQgAAUVyKpoHjI6kAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_t_y = my_bmm(batch_H.permute(0,2,1), my_y)\n",
    "hty = to_pil_image(h_t_y[0].view(32,32,3).permute(2,0,1))\n",
    "hty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prof' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[43mprof\u001b[49m\u001b[38;5;241m.\u001b[39mkey_averages(group_by_input_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prof' is not defined"
     ]
    }
   ],
   "source": [
    "K = prof.key_averages(group_by_input_shape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls                                                                      Input Shapes  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                                        aten::_coalesce         0.59%     115.999ms         2.68%     531.246ms     459.953us     180.882ms        21.05%     205.271ms     177.724us           0 b           0 b       2.46 Gb      -2.55 Gb          1155                                                                    [[4096, 4096]]  \n",
      "void at::native::apply::coalesceValuesKernel<float, ...         0.00%       0.000us         0.00%       0.000us       0.000us     100.288ms        11.67%     100.288ms      44.573us           0 b           0 b           0 b           0 b          2250                                                                                []  \n",
      "void at_cuda_detail::cub::DeviceMergeSortMergeKernel...         0.00%       0.000us         0.00%       0.000us       0.000us      86.971ms        10.12%      86.971ms       7.440us           0 b           0 b           0 b           0 b         11690                                                                                []  \n",
      "                                        aten::_coalesce         0.08%      15.467ms         1.83%     362.411ms       3.452ms      82.969ms         9.66%      92.560ms     881.528us           0 b           0 b       2.08 Gb      -2.43 Gb           105                                                                [[409856, 311296]]  \n",
      "void at_cuda_detail::cub::DeviceMergeSortBlockSortKe...         0.00%       0.000us         0.00%       0.000us       0.000us      74.511ms         8.67%      74.511ms      33.116us           0 b           0 b           0 b           0 b          2250                                                                                []  \n",
      "void at_cuda_detail::cub::DeviceMergeSortPartitionKe...         0.00%       0.000us         0.00%       0.000us       0.000us      73.744ms         8.58%      73.744ms       6.308us           0 b           0 b           0 b           0 b         11690                                                                                []  \n",
      "                                        aten::_coalesce         0.18%      35.138ms         0.79%     157.205ms     499.063us      64.424ms         7.50%      74.603ms     236.835us           0 b           0 b       1.18 Gb      -1.51 Gb           315                                                                  [[102400, 4096]]  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      58.365ms         6.79%      58.365ms      63.441us           0 b           0 b           0 b           0 b           920                                                                                []  \n",
      "                            aten::_sparse_sparse_matmul         0.13%      24.911ms         1.00%     197.083ms       1.877ms      46.470ms         5.41%      57.608ms     548.645us           0 b           0 b     525.00 Kb      -5.08 Mb           105                                                        [[4096, 256], [256, 4096]]  \n",
      "                                        aten::_coalesce         0.12%      23.854ms         1.69%     335.123ms       1.596ms      43.909ms         5.11%      50.678ms     241.322us           0 b           0 b     812.31 Mb      -1.02 Gb           210                                                                  [[4096, 102400]]  \n",
      "void cusparse::binary_search_lb_kernel<128u, 8u, 0ul...         0.00%       0.000us         0.00%       0.000us       0.000us      35.295ms         4.11%      35.295ms     164.161us           0 b           0 b           0 b           0 b           215                                                                                []  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      33.626ms         3.91%      33.626ms       3.739us           0 b           0 b           0 b           0 b          8992                                                                                []  \n",
      "                                              aten::cat         1.63%     322.684ms        19.06%        3.772s     541.456us      29.160ms         3.39%      51.730ms       7.426us      31.25 Mb      31.25 Mb      21.61 Gb      12.24 Gb          6966                                                                          [[], []]  \n",
      "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us      22.034ms         2.56%      22.034ms       9.881us           0 b           0 b           0 b           0 b          2230                                                                                []  \n",
      "                                            aten::copy_         0.00%     942.470us         0.16%      31.799ms     302.851us      21.761ms         2.53%      21.761ms     207.252us           0 b           0 b           0 b           0 b           105                                                    [[2, 204800], [2, 204800], []]  \n",
      "void thrust::THRUST_200302_500_600_700_750_800_860_9...         0.00%       0.000us         0.00%       0.000us       0.000us      20.603ms         2.40%      20.603ms       9.157us           0 b           0 b           0 b           0 b          2250                                                                                []  \n",
      "                            aten::_sparse_sparse_matmul         0.11%      22.371ms         1.05%     207.181ms       1.973ms      20.519ms         2.39%      72.443ms     689.934us           0 b           0 b     217.82 Mb      -1.19 Gb           105                                                  [[4096, 102400], [102400, 4096]]  \n",
      "                                            aten::copy_         0.01%       1.868ms         0.20%      38.976ms     185.599us      19.914ms         2.32%      19.914ms      94.829us           0 b           0 b           0 b           0 b           210                                                    [[2, 102400], [2, 102400], []]  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      18.580ms         2.16%      18.580ms       7.873us           0 b           0 b           0 b           0 b          2360                                                                                []  \n",
      "                                            aten::addmm         0.33%      65.884ms         3.74%     740.251ms     665.095us      18.432ms         2.15%     233.499ms     209.793us           0 b           0 b      52.17 Mb      -3.26 Gb          1113                                      [[4096, 3], [4096, 4096], [4096, 3], [], []]  \n",
      "void at_cuda_detail::cub::DeviceSelectSweepKernel<at...         0.00%       0.000us         0.00%       0.000us       0.000us      17.437ms         2.03%      17.437ms       3.873us           0 b           0 b           0 b           0 b          4502                                                                                []  \n",
      "                                          aten::nonzero         0.37%      73.908ms         0.89%     176.752ms      77.796us      17.253ms         2.01%      17.253ms       7.594us           0 b           0 b       1.11 Mb           0 b          2272                                                                             [[3]]  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      16.241ms         1.89%      16.241ms       3.478us           0 b           0 b           0 b           0 b          4670                                                                                []  \n",
      "                                          aten::nonzero         0.24%      48.436ms         0.58%     114.153ms      98.834us      16.010ms         1.86%      16.010ms      13.862us           0 b           0 b     871.33 Mb           0 b          1155                                                                         [[98852]]  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      15.882ms         1.85%      15.882ms       2.818us           0 b           0 b           0 b           0 b          5636                                                                                []  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      15.704ms         1.83%      15.704ms       3.474us           0 b           0 b           0 b           0 b          4520                                                                                []  \n",
      "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us      15.443ms         1.80%      15.443ms       6.329us           0 b           0 b           0 b           0 b          2440                                                                                []  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      15.159ms         1.76%      15.159ms       3.354us           0 b           0 b           0 b           0 b          4520                                                                                []  \n",
      "void convert_CooToCsr_kernel<0>(int const*, int, int...         0.00%       0.000us         0.00%       0.000us       0.000us      14.875ms         1.73%      14.875ms       6.670us           0 b           0 b           0 b           0 b          2230                                                                                []  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      14.851ms         1.73%      14.851ms       3.484us           0 b           0 b           0 b           0 b          4263                                                                                []  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      14.431ms         1.68%      14.431ms       9.784us           0 b           0 b           0 b           0 b          1475                                                                                []  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      13.263ms         1.54%      13.263ms       3.751us           0 b           0 b           0 b           0 b          3536                                                                                []  \n",
      "void cusparse::load_balancing_kernel<256u, 1u, 0ul, ...         0.00%       0.000us         0.00%       0.000us       0.000us      12.168ms         1.42%      12.168ms       6.922us           0 b           0 b           0 b           0 b          1758                                                                                []  \n",
      "                                           aten::arange         0.16%      32.458ms         0.40%      78.500ms      14.695us      11.913ms         1.39%      11.913ms       2.230us      15.63 Mb     800.70 Kb       2.58 Gb           0 b          5342                                                                 [[], [], [], [0]]  \n",
      "void (anonymous namespace)::elementwise_kernel_with_...         0.00%       0.000us         0.00%       0.000us       0.000us      11.913ms         1.39%      11.913ms       2.572us           0 b           0 b           0 b           0 b          4632                                                                                []  \n",
      "                                        aten::_coalesce         0.14%      28.029ms         0.57%     112.865ms     358.303us      11.392ms         1.33%      15.395ms      48.873us           0 b           0 b       1.54 Mb      -1.85 Mb           315                                                                     [[4096, 256]]  \n",
      "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us      11.356ms         1.32%      11.356ms       5.002us           0 b           0 b           0 b           0 b          2270                                                                                []  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.804ms         1.26%      10.804ms       2.400us           0 b           0 b           0 b           0 b          4502                                                                                []  \n",
      "void thrust::THRUST_200302_500_600_700_750_800_860_9...         0.00%       0.000us         0.00%       0.000us       0.000us      10.427ms         1.21%      10.427ms       2.317us           0 b           0 b           0 b           0 b          4500                                                                                []  \n",
      "                                            aten::copy_         0.01%       1.025ms         0.11%      22.204ms     153.128us       9.592ms         1.12%       9.592ms      66.149us           0 b           0 b           0 b           0 b           145                                                          [[204800], [204800], []]  \n",
      "                                              aten::mul         4.27%     845.509ms         4.44%     879.168ms     279.101us       9.574ms         1.11%       9.574ms       3.039us           0 b           0 b     145.69 Mb     145.69 Mb          3150                                                                [[], [1, 4096, 3]]  \n",
      "void cusparse::binary_search_partition_kernel<256, 2...         0.00%       0.000us         0.00%       0.000us       0.000us       9.003ms         1.05%       9.003ms       5.121us           0 b           0 b           0 b           0 b          1758                                                                                []  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.889ms         1.03%       8.889ms       2.167us           0 b           0 b           0 b           0 b          4102                                                                                []  \n",
      "                         Memcpy DtoH (Device -> Pinned)         0.00%       0.000us         0.00%       0.000us       0.000us       8.718ms         1.01%       8.718ms       1.828us           0 b           0 b           0 b           0 b          4769                                                                                []  \n",
      "                                     aten::index_select         0.07%      13.065ms         0.16%      31.779ms      27.515us       8.013ms         0.93%       8.013ms       6.938us           0 b           0 b       2.57 Gb           0 b          1155                                                         [[3, 98852], [], [98852]]  \n",
      "                                            aten::copy_         0.06%      12.641ms         0.14%      28.298ms      12.250us       7.974ms         0.93%       7.974ms       3.452us           0 b           0 b           0 b           0 b          2310                                                            [[98596], [98596], []]  \n",
      "                                          aten::nonzero         0.11%      21.543ms         0.26%      51.075ms      97.285us       7.764ms         0.90%       7.764ms      14.788us           0 b           0 b     951.01 Mb           0 b           525                                                                        [[200176]]  \n",
      "                                            aten::copy_         0.03%       5.586ms         0.10%      19.613ms      17.669us       7.410ms         0.86%       7.410ms       6.675us           0 b           0 b           0 b           0 b          1110                                                    [[1, 200176], [1, 200176], []]  \n",
      "                                             aten::div_         0.14%      27.780ms         0.21%      41.959ms      18.164us       7.339ms         0.85%       7.339ms       3.177us       1.17 Kb       1.17 Kb           0 b           0 b          2310                                                              [[1, 98596], [], []]  \n",
      "                                             aten::add_         0.06%      10.987ms         0.12%      23.553ms      10.196us       6.948ms         0.81%       6.948ms       3.008us           0 b           0 b           0 b           0 b          2310                                                      [[1, 98596], [1, 98596], []]  \n",
      "                                            aten::addmm         0.06%      11.953ms         0.59%     116.430ms     567.952us       6.926ms         0.81%      19.655ms      95.876us           0 b           0 b       9.61 Mb     -14.31 Mb           205                                        [[4096, 3], [4096, 256], [256, 3], [], []]  \n",
      "void at_cuda_detail::cub::DeviceCompactInitKernel<at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.873ms         0.80%       6.873ms       1.447us           0 b           0 b           0 b           0 b          4749                                                                                []  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us       6.730ms         0.78%       6.730ms       1.846us           0 b           0 b           0 b           0 b          3645                                                                                []  \n",
      "                                            aten::copy_         0.07%      13.131ms         0.17%      33.482ms      14.494us       6.474ms         0.75%       6.474ms       2.803us           0 b           0 b           0 b           0 b          2310                                                      [[1, 98596], [1, 98596], []]  \n",
      "                                     aten::index_select         0.05%      10.380ms         0.13%      24.848ms      21.513us       6.212ms         0.72%       6.212ms       5.379us           0 b           0 b       2.01 Gb           0 b          1155                                                             [[3, 98852], [], [2]]  \n",
      "                                     aten::index_select         0.03%       5.848ms         0.07%      14.074ms      26.808us       6.202ms         0.72%       6.202ms      11.814us           0 b           0 b       2.38 Gb           0 b           525                                                       [[3, 200176], [], [200176]]  \n",
      "                                     aten::index_select         0.01%       1.845ms        20.99%        4.155s      39.569ms       6.167ms         0.72%       6.167ms      58.738us           0 b           0 b       2.46 Gb           0 b           105                                                     [[3, 1015008], [], [1015008]]  \n",
      "void at_cuda_detail::cub::DeviceReduceSingleTileKern...         0.00%       0.000us         0.00%       0.000us       0.000us       5.670ms         0.66%       5.670ms       2.055us           0 b           0 b           0 b           0 b          2759                                                                                []  \n",
      "                                            aten::copy_         0.01%       2.616ms         0.09%      18.276ms      43.515us       5.583ms         0.65%       5.583ms      13.292us           0 b           0 b           0 b           0 b           420                                                          [[102400], [102400], []]  \n",
      "                                            aten::fill_         0.08%      14.908ms         0.17%      33.347ms      11.717us       5.362ms         0.62%       5.362ms       1.884us           0 b           0 b           0 b           0 b          2846                                                                   [[4096, 3], []]  \n",
      "void at_cuda_detail::cub::DeviceReduceKernel<at_cuda...         0.00%       0.000us         0.00%       0.000us       0.000us       5.341ms         0.62%       5.341ms       2.992us           0 b           0 b           0 b           0 b          1785                                                                                []  \n",
      "                                            aten::copy_         0.04%       7.795ms         0.09%      18.521ms      13.568us       5.113ms         0.60%       5.113ms       3.746us           0 b           0 b           0 b           0 b          1365                                                          [[200176], [200176], []]  \n",
      "                                            aten::addmm         0.05%       8.981ms         1.96%     388.602ms       3.701ms       4.983ms         0.58%     101.173ms     963.549us           0 b           0 b     495.53 Mb      -3.59 Gb           105                              [[409856, 3], [409856, 311296], [311296, 3], [], []]  \n",
      "void cusparse::matrix_scalar_multiply_kernel<cuspars...         0.00%       0.000us         0.00%       0.000us       0.000us       4.970ms         0.58%       4.970ms       2.668us           0 b           0 b           0 b           0 b          1863                                                                                []  \n",
      "                                     aten::index_select         0.07%      14.633ms         0.16%      32.287ms      27.954us       4.960ms         0.58%       4.960ms       4.295us           0 b           0 b     435.94 Mb           0 b          1155                                                            [[98852], [], [98852]]  \n",
      "                                              aten::add         3.03%     600.092ms         3.15%     623.532ms     269.927us       4.931ms         0.57%       5.117ms       2.215us           0 b           0 b     108.28 Mb     105.09 Mb          2310                                                  [[1, 4096, 3], [1, 4096, 3], []]  \n",
      "                                            aten::copy_         0.06%      11.005ms         0.10%      19.864ms      13.959us       4.855ms         0.57%       4.855ms       3.412us           0 b           0 b           0 b           0 b          1423                                                        [[3, 4096], [3, 4096], []]  \n",
      "                                            aten::copy_         0.05%       9.693ms         0.09%      17.445ms      12.260us       4.691ms         0.55%       4.691ms       3.296us           0 b           0 b           0 b           0 b          1423                                                        [[4096, 3], [4096, 3], []]  \n",
      "                                        aten::_coalesce         0.06%      11.225ms         0.31%      60.856ms     486.849us       4.608ms         0.54%       6.199ms      49.595us           0 b           0 b     625.00 Kb    -750.00 Kb           125                                                                     [[256, 4096]]  \n",
      "                                              aten::add         1.64%     324.155ms         1.70%     336.081ms     246.213us       4.552ms         0.53%       4.552ms       3.335us           0 b           0 b       1.81 Gb       1.81 Gb          1365                                              [[1, 102400, 3], [1, 102400, 3], []]  \n",
      "                                            aten::addmm         0.06%      12.140ms         0.75%     147.760ms     703.618us       4.453ms         0.52%      58.263ms     277.444us           0 b           0 b     287.15 Mb      -1.44 Gb           210                                  [[102400, 3], [102400, 4096], [4096, 3], [], []]  \n",
      "                                        aten::_coalesce         0.05%       9.545ms         0.10%      20.251ms       1.013ms       4.204ms         0.49%       5.131ms     256.556us           0 b           0 b     109.92 Mb     -96.20 Mb            20                                                               [[1, 4096, 102400]]  \n",
      "                                             aten::div_         0.07%      13.491ms         0.11%      21.775ms      19.617us       4.089ms         0.48%       4.089ms       3.684us         456 b         456 b           0 b           0 b          1110                                                             [[1, 200176], [], []]  \n",
      "void cusparse::binary_search_lb_offset_kernel<128u, ...         0.00%       0.000us         0.00%       0.000us       0.000us       4.040ms         0.47%       4.040ms      18.790us           0 b           0 b           0 b           0 b           215                                                                                []  \n",
      "                                             aten::add_         0.03%       5.345ms         0.06%      11.268ms      10.152us       4.016ms         0.47%       4.016ms       3.618us           0 b           0 b           0 b           0 b          1110                                                    [[1, 200176], [1, 200176], []]  \n",
      "void thrust::THRUST_200302_500_600_700_750_800_860_9...         0.00%       0.000us         0.00%       0.000us       0.000us       3.994ms         0.46%       3.994ms       1.775us           0 b           0 b           0 b           0 b          2250                                                                                []  \n",
      "void at_cuda_detail::cub::DeviceReduceSingleTileKern...         0.00%       0.000us         0.00%       0.000us       0.000us       3.946ms         0.46%       3.946ms       2.088us           0 b           0 b           0 b           0 b          1890                                                                                []  \n",
      "                                              aten::mul         1.10%     217.571ms         1.14%     225.292ms     268.205us       3.843ms         0.45%       3.843ms       4.575us           0 b           0 b       1.09 Gb       1.09 Gb           840                                                             [[1], [1, 102400, 3]]  \n",
      "                                               aten::ne         0.17%      33.647ms         0.24%      46.673ms      20.543us       3.837ms         0.45%       3.837ms       1.689us           0 b           0 b       1.11 Mb       1.11 Mb          2272                                                                         [[3], []]  \n",
      "void cub::CUB_200101_500_520_600_610_700_750_800_860...         0.00%       0.000us         0.00%       0.000us       0.000us       3.485ms         0.41%       3.485ms       3.242us           0 b           0 b           0 b           0 b          1075                                                                                []  \n",
      "                                          aten::nonzero         0.08%      15.124ms         0.18%      36.558ms      83.085us       3.379ms         0.39%       3.379ms       7.679us           0 b           0 b     880.00 Kb           0 b           440                                                                           [[256]]  \n",
      "                                     aten::index_select         0.02%       4.802ms         1.13%     223.402ms     425.528us       3.375ms         0.39%       3.375ms       6.428us           0 b           0 b       1.59 Gb           0 b           525                                                            [[3, 200176], [], [2]]  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.349ms         0.39%       3.349ms       1.969us           0 b           0 b           0 b           0 b          1701                                                                                []  \n",
      "void cusparse::binary_search_partition_kernel<128, 1...         0.00%       0.000us         0.00%       0.000us       0.000us       3.221ms         0.37%       3.221ms       3.745us           0 b           0 b           0 b           0 b           860                                                                                []  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.147ms         0.37%       3.147ms       2.922us           0 b           0 b           0 b           0 b          1077                                                                                []  \n",
      "                                               aten::eq         0.12%      23.824ms         0.19%      37.103ms      32.124us       3.113ms         0.36%       3.113ms       2.695us           0 b           0 b     109.41 Mb     109.41 Mb          1155                                                                     [[98852], []]  \n",
      "void cub::CUB_200101_500_520_600_610_700_750_800_860...         0.00%       0.000us         0.00%       0.000us       0.000us       3.102ms         0.36%       3.102ms      14.426us           0 b           0 b           0 b           0 b           215                                                                                []  \n",
      "                                     aten::index_select         0.03%       6.575ms         0.09%      17.806ms      33.916us       3.049ms         0.35%       3.049ms       5.808us           0 b           0 b     400.93 Mb           0 b           525                                                          [[200176], [], [200176]]  \n",
      "                                            aten::copy_         0.01%       1.138ms         0.02%       3.828ms      18.229us       3.017ms         0.35%       3.017ms      14.368us           0 b           0 b           0 b           0 b           210                                                  [[1, 1015008], [1, 1015008], []]  \n",
      "                                     aten::index_select         0.01%       1.501ms         4.98%     986.388ms       9.394ms       2.958ms         0.34%       2.958ms      28.174us           0 b           0 b       1.62 Gb           0 b           105                                                           [[3, 1015008], [], [2]]  \n",
      "                                              aten::sub         1.67%     330.606ms         1.73%     342.753ms     272.026us       2.918ms         0.34%       2.961ms       2.350us           0 b           0 b      59.06 Mb      58.08 Mb          1260                                                  [[1, 4096, 3], [1, 4096, 3], []]  \n",
      "                                              aten::sub         0.83%     164.768ms         0.87%     171.214ms     232.945us       2.810ms         0.33%       2.810ms       3.823us           0 b           0 b    1014.55 Mb    1014.55 Mb           735                                              [[1, 102400, 3], [1, 102400, 3], []]  \n",
      "void thrust::THRUST_200302_500_600_700_750_800_860_9...         0.00%       0.000us         0.00%       0.000us       0.000us       2.654ms         0.31%       2.654ms      10.329us           0 b           0 b           0 b           0 b           257                                                                                []  \n",
      "                                          aten::nonzero         0.03%       5.505ms         0.07%      13.602ms     129.538us       2.628ms         0.31%       2.628ms      25.032us           0 b           0 b       1.67 Mb           0 b           105                                                                  [[1, 204800, 3]]  \n",
      "                                             aten::add_         0.01%       1.114ms         0.01%       2.244ms      10.687us       2.628ms         0.31%       2.628ms      12.513us           0 b           0 b           0 b           0 b           210                                                  [[1, 1015008], [1, 1015008], []]  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us       2.554ms         0.30%       2.554ms       6.081us           0 b           0 b           0 b           0 b           420                                                                                []  \n",
      "                                  aten::scatter_reduce_         0.00%     593.451us         0.01%       1.448ms      72.394us       2.415ms         0.28%       2.523ms     126.151us           0 b           0 b           0 b           0 b            20                                          [[4096], [], [204800], [204800], [], []]  \n",
      "void at::native::_scatter_gather_elementwise_kernel<...         0.00%       0.000us         0.00%       0.000us       0.000us       2.415ms         0.28%       2.415ms     120.748us           0 b           0 b           0 b           0 b            20                                                                                []  \n",
      "                                             aten::div_         0.01%       2.950ms         0.02%       4.330ms      20.617us       2.415ms         0.28%       2.415ms      11.499us          64 b          64 b           0 b           0 b           210                                                            [[1, 1015008], [], []]  \n",
      "                                          aten::nonzero         0.03%       4.980ms         0.06%      11.959ms     113.891us       2.333ms         0.27%       2.333ms      22.220us           0 b           0 b     813.13 Mb           0 b           105                                                                       [[1015008]]  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "Self CPU time total: 19.790s\n",
      "Self CUDA time total: 859.147ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(K.table(sort_by=\"self_cuda_time_total\", row_limit=100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
