{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6zipHPScsqP",
        "outputId": "1c43c348-3b84-4c3c-c8c0-e2c9d71302b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eRZYU0adIe1",
        "outputId": "d3eda1a8-1936-4c38-c47c-8e1626001a31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!unzip DIV2K_train_HR.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imefbTh6kufr",
        "outputId": "43ef6424-60b0-4386-ab12-d2dea14d0787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5.1+cxx11.abi\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SSmNfT9pD1J"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import tracemalloc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9f6wyNMbsZN"
      },
      "outputs": [],
      "source": [
        "def image_graph_edges(H, W, k=5):\n",
        "    \"\"\"\n",
        "    Generates an edges tensor for a graph where each pixel in an HxW image\n",
        "    is connected to its kxk nearest neighbors, shifting the window at borders.\n",
        "\n",
        "    Args:\n",
        "    - H (int): Image height\n",
        "    - W (int): Image width\n",
        "    - k (int): Neighborhood size (kxk)\n",
        "    - device (str): 'cuda' or 'cpu' for tensor allocation\n",
        "\n",
        "    Returns:\n",
        "    - torch.Tensor: Edges tensor of shape (num_edges, 2)\n",
        "    \"\"\"\n",
        "\n",
        "    half_k = k // 2\n",
        "\n",
        "    # Create grid of pixel indices\n",
        "    row_indices = torch.arange(H)\n",
        "    col_indices = torch.arange(W)\n",
        "\n",
        "    grid_r, grid_c = torch.meshgrid(row_indices, col_indices, indexing='ij')\n",
        "    pixel_indices = grid_r * W + grid_c  # Convert (row, col) to 1D index\n",
        "\n",
        "    # Create all possible shifts within the kxk neighborhood\n",
        "    d_row = torch.arange(-half_k, half_k + 1)\n",
        "    d_col = torch.arange(-half_k, half_k + 1)\n",
        "\n",
        "    shift_r, shift_c = torch.meshgrid(d_row, d_col, indexing='ij')\n",
        "    shift_r = shift_r.flatten()\n",
        "    shift_c = shift_c.flatten()\n",
        "\n",
        "    # Compute neighbor locations\n",
        "    neighbor_r = grid_r.unsqueeze(-1) + shift_r\n",
        "    neighbor_c = grid_c.unsqueeze(-1) + shift_c\n",
        "\n",
        "    # Shift window for boundary pixels\n",
        "    neighbor_r = neighbor_r.clamp(0, H - 1)\n",
        "    neighbor_c = neighbor_c.clamp(0, W - 1)\n",
        "\n",
        "    # Convert to 1D indices\n",
        "    neighbor_indices = neighbor_r * W + neighbor_c\n",
        "\n",
        "    # Create edges\n",
        "    edges = torch.stack([\n",
        "        pixel_indices.unsqueeze(-1).expand(-1, -1, k * k),  # Expand for each neighbor\n",
        "        neighbor_indices\n",
        "    ], dim=-1)\n",
        "\n",
        "    return edges.reshape(-1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G14bNgwYzQmy"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceKMTFqpGH24"
      },
      "outputs": [],
      "source": [
        "# from torch_geometric.utils import softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo3cmnOUqaQE"
      },
      "outputs": [],
      "source": [
        "import torch.sparse\n",
        "import gc\n",
        "\n",
        "class GraphLearning(nn.Module):\n",
        "    def __init__(self, feature_dim, edges_p_node, device='cuda'):\n",
        "        super(GraphLearning, self).__init__()\n",
        "        self.M = nn.Parameter(torch.eye(feature_dim, feature_dim, device=device) * 1.5)\n",
        "        self.edges_p_node = edges_p_node\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x, edges):\n",
        "        \"\"\"\n",
        "        let B = batch_size, N = num_nodes, M = num_edges, K = sample_size, C = num_channels, F = feature_dim\n",
        "        x: tensor of features for each graph (B, N, F)\n",
        "        edges: tensor of edge indices for each graph (B, M, 2)\n",
        "        return: incidence matrix C for each graph (B, N, M)\n",
        "        \"\"\"\n",
        "        batch_size, num_nodes, _ = x.shape\n",
        "        num_edges = edges.shape[1]\n",
        "\n",
        "        device = self.device\n",
        "\n",
        "        source_x = x[torch.arange(batch_size).unsqueeze(1), edges[:, :, 0], :] # source_x (B, M, F)\n",
        "        target_x = x[torch.arange(batch_size).unsqueeze(1), edges[:, :, 1], :] # target_x (B, M, F)\n",
        "        d = target_x - source_x # d (B, M, F)\n",
        "        e = torch.einsum(\"bik,kl,bil->bi\", d, self.M, d) # e (B, M)\n",
        "\n",
        "        batch_indices = torch.arange(batch_size).unsqueeze(1).repeat(2, num_edges).view(-1)\n",
        "        # batch_indices (B * 2M, )\n",
        "        row_indices = torch.cat([edges[:, :, 0], edges[:, :, 1]]).view(-1)\n",
        "        # row_indices (B * 2M, )\n",
        "        col_indices = torch.cat([torch.arange(num_edges)] * 2).repeat(batch_size,1).view(-1)\n",
        "        # col_indices (B * 2M, )\n",
        "        idxs = torch.stack([batch_indices.to(device), row_indices, col_indices.to(device)], dim=0)\n",
        "        # idxs (3, B * 2M)\n",
        "\n",
        "        vals = torch.cat([e, -e]).view(-1) # vals (B * 2M, )\n",
        "        # vals = softmax(vals, row_indices) # vals (B * 2M, )\n",
        "\n",
        "        C = torch.sparse_coo_tensor(idxs, vals, size=(batch_size, num_nodes, num_edges), device=device, ).coalesce()\n",
        "        # C = C.to_sparse_csr()\n",
        "        with torch.no_grad():\n",
        "            del batch_indices, row_indices, col_indices, idxs, vals, source_x, target_x, d, e\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        C = torch.sparse.softmax(C, dim=-1)\n",
        "        # C sparse(B, M, N)\n",
        "        return C\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO9H5KsD9XR0"
      },
      "outputs": [],
      "source": [
        "import torch.sparse\n",
        "\n",
        "def my_bmm(M1, M2):\n",
        "    assert len(M1.shape)==3 and len(M2.shape)==3\n",
        "    O = torch.stack([torch.sparse.mm(M1[i], M2[i]) for i in range(M1.shape[0])],dim=0)\n",
        "    if O.is_sparse:\n",
        "        return O.coalesce()\n",
        "    else:\n",
        "        return O\n",
        "\n",
        "class SparseCache:\n",
        "    eyes_c = {}\n",
        "    zeros_c = {}\n",
        "    device = 'cpu'\n",
        "    @staticmethod\n",
        "    def set_device(device):\n",
        "        if(SparseCache.device != device):\n",
        "            SparseCache.eyes_c = {}\n",
        "            SparseCache.zeros_C = {}\n",
        "        SparseCache.device = device\n",
        "\n",
        "    @staticmethod\n",
        "    def _sparse_eye(S):\n",
        "        return torch.sparse.spdiags(torch.ones(S),torch.zeros(1, dtype=int),(S,S)).to(SparseCache.device)\n",
        "\n",
        "    @staticmethod\n",
        "    def eye_(S):\n",
        "        SparseCache.eyes_c[(S,S)] = SparseCache.eyes_c.get((S,S),SparseCache._sparse_eye(S))\n",
        "        return SparseCache.eyes_c[(S,S)]\n",
        "\n",
        "    def eye(B,S):\n",
        "        SparseCache.eyes_c[(B,S,S)] = SparseCache.eyes_c.get((B,S,S),torch.stack([SparseCache.eye_(S) for _ in range(B)], dim=0).detach())\n",
        "        return SparseCache.eyes_c[(B,S,S)]\n",
        "\n",
        "    def zeros(S):\n",
        "        SparseCache.zeros_c[S] = SparseCache.zeros_c.get(S,torch.sparse_coo_tensor(size=S, device=SparseCache.device).detach())\n",
        "        return SparseCache.zeros_c[S]\n",
        "\n",
        "\n",
        "class ADMMLayer(nn.Module):\n",
        "    def __init__(self, cg_steps, edges_p_node, sample_size, device):\n",
        "        super(ADMMLayer, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.rand(1, device=device))\n",
        "        self.alpha = nn.Parameter(torch.rand(cg_steps, device=device))\n",
        "        self.beta = nn.Parameter(torch.rand(cg_steps, device=device))\n",
        "        self.cg_steps = cg_steps\n",
        "        self.edges_p_node = edges_p_node\n",
        "        self.sample_size = sample_size\n",
        "\n",
        "        self.device = device\n",
        "        SparseCache.set_device(device)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, C, H, y, x, q_tilde, mu):\n",
        "        \"\"\"\n",
        "        let B = batch_size, N = num_nodes, M = num_edges, K = sample_size, C = num_channels\n",
        "        C: incidence matrix of size (B, N, M)\n",
        "        H: sampling matrix of size (B, K, N)\n",
        "        y: signal to interpolate of size (B, K, C)\n",
        "        x: initial guess of size (B, N, C)\n",
        "        q_tilde: initial guess of size (B, 2M, C)\n",
        "        mu: initial guess of size (B, 4M + K, C)\n",
        "        \"\"\"\n",
        "        K = self.sample_size\n",
        "        B, N, channels = x.shape\n",
        "        _, _, M = C.shape\n",
        "        gamma = self.gamma\n",
        "        device = C.device\n",
        "        mu_a, mu_b, mu_c, mu_d, mu_e = mu.narrow(1, 0, M), mu.narrow(1, M, M), \\\n",
        "                                       mu.narrow(1, 2*M, K), mu.narrow(1, 2*M + K, M), \\\n",
        "                                       mu.narrow(1, 3*M + K, M)\n",
        "        # mu_a (B,M,C), mu_b (B,M,C), mu_c (B,K,C), mu_d (B,M,C), mu_e (B,M,C)\n",
        "\n",
        "        q_tilde_1, q_tilde_2 = q_tilde.narrow(1, 0, M), q_tilde.narrow(1, M, M)\n",
        "        # q_tilde_1 (B,M,C), q_tilde_2 (B,M,C)\n",
        "\n",
        "        z_n = -1 / gamma * torch.ones(B,M,channels, device=device) \\\n",
        "              - 1 / (2 * gamma) * (mu_a + mu_b + mu_d + mu_e) \\\n",
        "              + 1 / 2 * (q_tilde_1 + q_tilde_2)\n",
        "        # z_n (B, M, C)\n",
        "\n",
        "        H_T = H.permute(0,2,1)\n",
        "\n",
        "        b_cg = 1 / (2 * gamma) * my_bmm(C, (mu_a - mu_b + mu_d - mu_e)) \\\n",
        "            - 1 / gamma * my_bmm(H_T, mu_c) - 1 / 2 * torch.bmm(C, (q_tilde_1 - q_tilde_2)) \\\n",
        "            + my_bmm(H_T, y)\n",
        "        # b_cg (B, N, C)\n",
        "\n",
        "        C_T = C.permute(0,2,1)\n",
        "\n",
        "        L = my_bmm(H_T, H) + my_bmm(C,C_T)\n",
        "        # L = SparseCache.eye(B,N)\n",
        "        # L (B, N, N)\n",
        "\n",
        "        g = - b_cg + my_bmm(L, x)\n",
        "        # g (B, N, C)\n",
        "\n",
        "        v = torch.zeros_like(x)\n",
        "        # v (B, N, C)\n",
        "\n",
        "        for t in range(self.cg_steps):\n",
        "            x, v, g = self.cg_update(L, t, x, v, g)\n",
        "\n",
        "        q_1 = 1 / 2 * (z_n - my_bmm(C_T, x)) + 1 / (2 * gamma) * \\\n",
        "              (mu_a - mu_d + gamma * q_tilde_1)\n",
        "        # q_1 (B, M, C)\n",
        "        q_2 = 1 / 2 * (z_n + my_bmm(C_T, x)) + 1 / (2 * gamma) * \\\n",
        "              (mu_b - mu_e + gamma * q_tilde_2)\n",
        "        # q_2 (B, M, C)\n",
        "\n",
        "        q = torch.cat([q_1, q_2], dim=1)\n",
        "        # q (B, 2M, C)\n",
        "\n",
        "        q_tilde_1_n = q_1 + 1 / (gamma) * mu_d\n",
        "        q_tilde_1_n = torch.maximum(q_tilde_1_n, torch.zeros_like(q_tilde_1_n, device=device))\n",
        "        q_tilde_2_n = q_2 + 1 / (gamma) * mu_e\n",
        "        q_tilde_2_n = torch.maximum(q_tilde_2_n, torch.zeros_like(q_tilde_2_n, device=device))\n",
        "\n",
        "        q_tilde_n = torch.cat([q_tilde_1_n, q_tilde_2_n], dim=1)\n",
        "        # q_tilde_n (B, 2M, C)\n",
        "\n",
        "        A = self.build_A(B, K, M, C_T, H, device)\n",
        "\n",
        "        B_mat = self.build_B_mat(B, M, N, A, device)\n",
        "\n",
        "        zxq = torch.cat([z_n, x, q], dim=1)\n",
        "        # zxq (B, M+N+2M, C)\n",
        "\n",
        "        b = self.build_b(B,M,channels,y,device)\n",
        "        # b (B, 2M + K, C)\n",
        "        bq_tilde = torch.cat([b, q_tilde_n.to_sparse_coo()], dim=1)\n",
        "        # bq_tilde (B, 2M + K + 2M, C)\n",
        "\n",
        "        mu_n = mu + gamma * (my_bmm(B_mat,zxq) - bq_tilde)\n",
        "        # mu_n (B, 2M+K+2M, C)\n",
        "        with torch.no_grad():\n",
        "            del B_mat, zxq, b, bq_tilde\n",
        "            del q_1, q_2, q_tilde_1_n, q_tilde_2_n\n",
        "            del A, L, g, v, z_n, b_cg\n",
        "            del mu_a, mu_b, mu_c, mu_d, mu_e\n",
        "            del C_T, q_tilde_1, q_tilde_2\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        return x, q_tilde_n, mu_n\n",
        "\n",
        "\n",
        "    def cg_update(self, L, t, x, v, g):\n",
        "        \"\"\"\n",
        "        Conjugate gradient update step.\n",
        "        L: (N, N)\n",
        "        t: int\n",
        "        x: (B, N, C)\n",
        "        v: (B, N, C)\n",
        "        g: (B, N, C)\n",
        "        \"\"\"\n",
        "        a = self.alpha[t]\n",
        "        b = self.beta[t]\n",
        "        g_n = g - a * my_bmm(L, v)\n",
        "        # g_n (B, N, C)\n",
        "        v_n = g_n + b * v\n",
        "        # v_n (B, N, C)\n",
        "        x_n = - a * v_n + x\n",
        "        # x_n (B, N, C)\n",
        "        return x_n, v_n, g_n\n",
        "\n",
        "    def build_A(self,B,K,M,C_T,H,device):\n",
        "        I_S = lambda S: SparseCache.eye(B, S)\n",
        "        I_M = I_S(M) # (M, M)\n",
        "        Z_S = lambda *S: SparseCache.zeros(S)\n",
        "\n",
        "        A_row_0 = torch.cat([I_M, -C_T, -I_M, Z_S(B,M,M)], dim=2)\n",
        "        # (B, M, M+N+M+M)\n",
        "        A_row_1 = torch.cat([I_M, C_T, Z_S(B,M,M), -I_M], dim=2)\n",
        "        # (B, M, M+N+M+M)\n",
        "        A_row_2 = torch.cat([Z_S(B, K, M), H, Z_S(B, K, 2*M)], dim=2)\n",
        "        # (B, K, M+N+M+M)\n",
        "        A = torch.cat([A_row_0, A_row_1, A_row_2], dim=1).detach()\n",
        "        # A (B, M+M+K, M+N+M+M)\n",
        "        del A_row_0, A_row_1, A_row_2\n",
        "        return A\n",
        "\n",
        "    def build_B_mat(self,B,M,N,A,device):\n",
        "        I_S = lambda S: SparseCache.eye(B, S)\n",
        "        I_M = I_S(M) # (M, M)\n",
        "        Z_S = lambda *S: SparseCache.zeros(S)\n",
        "        B_lower_block = torch.cat([Z_S(B, 2 * M, M + N), I_S(2*M)],dim=2)\n",
        "        # (B, 2M, M+N+M+M)\n",
        "        B_mat = torch.cat([A, B_lower_block], dim=1).detach()\n",
        "        # B (B, 2M+K+2M, M+N+2M)\n",
        "        del B_lower_block\n",
        "        return B_mat\n",
        "\n",
        "    def build_b(self,B,M,channels,y,device):\n",
        "        Z_S = lambda *S: SparseCache.zeros(S)\n",
        "        b = torch.cat([Z_S(B,M,channels),Z_S(B,M,channels),y.to_sparse_coo()], dim=1)\n",
        "        # b (B, 2M + K, C)\n",
        "        return b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zlk7NCofG8ky"
      },
      "outputs": [],
      "source": [
        "class ADMMBlock(nn.Module):\n",
        "    def __init__(self, num_admm_layers, cg_steps, edges_p_node, sample_size, device='cuda'):\n",
        "        super(ADMMBlock, self).__init__()\n",
        "\n",
        "        self.num_admm_layers = num_admm_layers\n",
        "        self.cg_steps = cg_steps\n",
        "        self.num_edges = edges_p_node\n",
        "        self.sample_size = sample_size\n",
        "\n",
        "        self.admm_layers = nn.ModuleList([ADMMLayer(cg_steps, edges_p_node, sample_size, device) for _ in range(num_admm_layers)])\n",
        "        self.sample_size = sample_size\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, C, H, y):\n",
        "        \"\"\"\n",
        "        let B = batch_size, N = num_nodes, M = num_edges, K = sample_size, C = num_channels\n",
        "        C: incidence matrix of size (B, N, M)\n",
        "        H: sampling matrix of size (B, K, N)\n",
        "        y: signal to interpolate of size (B, K, C)\n",
        "        \"\"\"\n",
        "        batch_size, num_nodes, num_edges = C.shape\n",
        "        channels = y.shape[-1]\n",
        "        device = self.device\n",
        "\n",
        "        x = torch.sparse_coo_tensor(size=(batch_size, num_nodes, channels), device=device)\n",
        "        # x (B, N, C)\n",
        "        q_tilde = torch.sparse_coo_tensor(size=(batch_size, 2 * num_edges, channels), device=device).to_dense()\n",
        "        # q_tilde (B, 2M, C)\n",
        "        mu = torch.ones(batch_size, 4 * num_edges + self.sample_size, channels, device=device) * 0.1\n",
        "        # mu (B, 4M+K, C)\n",
        "        i = 0\n",
        "        for admm_layer in self.admm_layers:\n",
        "            i += 1\n",
        "            x, q_tilde, mu = admm_layer(C, H, y, x, q_tilde, mu)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df-H_ivxXGe8"
      },
      "outputs": [],
      "source": [
        "class SkipConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding)\n",
        "\n",
        "        self.shortcut = nn.Identity() if in_channels == out_channels else \\\n",
        "                        nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.shortcut.requires_grad_(False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x) + self.shortcut(x)\n",
        "\n",
        "class ShallowCNN(nn.Module):\n",
        "    def __init__(self, in_channels, num_heads, channels_per_head):\n",
        "        super(ShallowCNN, self).__init__()\n",
        "        self.out_channels = num_heads * channels_per_head\n",
        "        self.channels_per_head  = channels_per_head\n",
        "        self.num_heads = num_heads\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            SkipConv(self.out_channels, self.out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            SkipConv(self.out_channels, self.out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            SkipConv(self.out_channels, self.out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        let B = batch_size, N = num_nodes = H * W, M = num_edges, K = sample_size, C = num_channels\n",
        "        x: tensor of values for each graph (B, H, W, C)\n",
        "        \"\"\"\n",
        "        # x: [B, C, H, W]\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        features = self.conv_layers(x)  # [B, 48, H, W]\n",
        "        # Split into 4 heads of 12 channels each\n",
        "        heads = torch.split(features, self.channels_per_head, dim=1)  # Each head: [B, 12, H, W]\n",
        "        heads = [head.permute(0,2,3,1) for head in heads]\n",
        "        return heads\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDyS6gVrT5Uz"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "class GraphLearnBlock(nn.Module):\n",
        "    def __init__(self, in_channels, feature_dim, edges_p_node, num_heads, num_admm_layers, cg_steps, sample_size, device='cuda'):\n",
        "        super(GraphLearnBlock, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.feature_dim = feature_dim\n",
        "        self.edges_p_node = edges_p_node\n",
        "        self.num_heads = num_heads\n",
        "        self.num_admm_layers = num_admm_layers\n",
        "        self.cg_steps = cg_steps\n",
        "        self.sample_size = sample_size\n",
        "\n",
        "        self.shallow_cnn = ShallowCNN(in_channels, num_heads, feature_dim)\n",
        "        self.graph_learnings = nn.ModuleList([GraphLearning(feature_dim, edges_p_node, device) for _ in range(num_heads)])\n",
        "        self.admm_block = ADMMBlock(num_admm_layers, cg_steps, edges_p_node, sample_size, device)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x, edges, H):\n",
        "        \"\"\"\n",
        "        let B = batch_size, N = num_nodes = H * W, M = num_edges, K = sample_size, C = num_channels\n",
        "        x: predicted tensor of values for each graph (B, H, W, C)\n",
        "        edges: tensor of edge indices for each graph (B, M, 2)\n",
        "        H: sampling matrix of size (B, K, N)\n",
        "        return: new predicted tensor of values for each graph (B, H, W, C)\n",
        "        \"\"\"\n",
        "        batch_size, Ht, Wi, channels = x.shape\n",
        "        features = self.shallow_cnn(x) # features (B, H, W, feature_dim)\n",
        "\n",
        "        x = x.view(batch_size, Ht * Wi, -1)\n",
        "        predicted_xs = []\n",
        "        for i in range(self.num_heads):\n",
        "            feat = features[i].view(batch_size, Ht * Wi, -1)\n",
        "            C = self.graph_learnings[i](feat, edges)\n",
        "            # C sparse(B, N, M)\n",
        "            y = my_bmm(H, x)\n",
        "            # y (B, K, C)\n",
        "            predicted_x = self.admm_block(C, H, y)\n",
        "            # predicted_x (B, N, C)\n",
        "            predicted_xs.append(predicted_x)\n",
        "            del predicted_x, y\n",
        "        x_tensor = torch.stack(predicted_xs, dim=-1)\n",
        "        del predicted_xs\n",
        "        # average over stacked dimension\n",
        "        if x_tensor.is_sparse:\n",
        "            x_tensor = x_tensor.coalesce()\n",
        "            x_tensor = x_tensor.to_dense()\n",
        "        ret =  torch.mean(x_tensor, dim=-1).view(batch_size, Ht, Wi, -1) # (B, N, C)\n",
        "        del x_tensor\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        return ret\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZu2N0WYbq0X"
      },
      "outputs": [],
      "source": [
        "class uGLR(nn.Module):\n",
        "    def __init__(self, in_channels, feature_dim, edges_p_node, num_heads, num_admm_layers, cg_steps, sample_size, num_learn_blocks, device='cuda'):\n",
        "        super(uGLR, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.feature_dim = feature_dim\n",
        "        self.edges_p_node = edges_p_node\n",
        "        self.num_heads = num_heads\n",
        "        self.num_admm_layers = num_admm_layers\n",
        "        self.cg_steps = cg_steps\n",
        "        self.sample_size = sample_size\n",
        "        self.num_learn_blocks = num_learn_blocks\n",
        "\n",
        "        self.graph_learn_blocks = nn.ModuleList([GraphLearnBlock(in_channels, feature_dim, edges_p_node, num_heads, num_admm_layers, cg_steps, sample_size, device) for _ in range(num_learn_blocks)])\n",
        "        self.device = device\n",
        "        self.first_admm = ADMMBlock(num_admm_layers, cg_steps, edges_p_node, sample_size, device)\n",
        "\n",
        "    def forward(self, y, edges, H, C, og_size):\n",
        "        \"\"\"\n",
        "        let B = batch_size, N = num_nodes, M = num_edges, K = sample_size, C = num_channels\n",
        "        y: tensor of values for each graph (B, K, C)\n",
        "        edges: tensor of edge indices for each graph (B, M, 2)\n",
        "        H: sampling matrix of size (B, K, N)\n",
        "        C: incidence matrix of size (B, N, M)\n",
        "        return: tensor of interpolated features x (B, N, C)\n",
        "        \"\"\"\n",
        "        B, K, channels = y.shape\n",
        "        x = self.first_admm(C, H, y) # (B, N, C)\n",
        "        Wi, Ht = og_size\n",
        "        if x.is_sparse:\n",
        "            x = x.coalesce()\n",
        "            x = x.to_dense()\n",
        "        x = x.view(B, Ht, Wi, -1)\n",
        "        for i in range(self.num_learn_blocks):\n",
        "            x = self.graph_learn_blocks[i](x, edges, H) # (B, N, C)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kyvhqqor8hI3"
      },
      "outputs": [],
      "source": [
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVK5L1Qnizga"
      },
      "outputs": [],
      "source": [
        "batch_size = 3\n",
        "num_batches = 100\n",
        "data_size = batch_size * num_batches\n",
        "img_w = 64\n",
        "img_h = 64\n",
        "channels = 3\n",
        "num_samples = 256\n",
        "neighbor_n = 5\n",
        "\n",
        "feature_dim = 12\n",
        "node_neighbors = neighbor_n * neighbor_n\n",
        "\n",
        "num_heads = 4\n",
        "num_admm_layers = 5\n",
        "num_cg_steps = 10\n",
        "num_gl_blocks = 5\n",
        "\n",
        "E = image_graph_edges(img_w,img_h,neighbor_n)\n",
        "\n",
        "X = torch.randn((data_size, img_w, img_h, channels))\n",
        "\n",
        "H = torch.zeros(img_w, img_h, num_samples)\n",
        "for i in range(num_samples):\n",
        "    x = i % 8\n",
        "    y = i // 8\n",
        "    H[x,y,i] = 1\n",
        "H = H.view(img_w*img_h,num_samples).T\n",
        "batch_H = H.repeat(batch_size,1,1).to_sparse_coo().detach()\n",
        "H = H.repeat(data_size,1,1).to_sparse_coo().detach()\n",
        "\n",
        "X_F = X.view(data_size, img_w * img_h, channels)\n",
        "Y_F = my_bmm(H, X_F)\n",
        "\n",
        "# E = E.repeat(data_size,1,1).detach()\n",
        "batch_E = E.repeat(batch_size,1,1).detach()\n",
        "\n",
        "g = GraphLearning(channels,node_neighbors,'cpu')\n",
        "\n",
        "X_F = X_F\n",
        "batch_E = batch_E\n",
        "\n",
        "batch_C = g(X_F[:batch_size], batch_E)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qccbabmizgb",
        "outputId": "3282b825-3ad2-4901-8147-a6d319203779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/cuda/memory.py:417: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "\n",
        "gc.collect()  # Force garbage collection\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_max_memory_allocated()\n",
        "torch.cuda.reset_max_memory_cached()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsdjIFZ1izgb"
      },
      "outputs": [],
      "source": [
        "model = uGLR(channels,num_heads,node_neighbors,num_heads,num_admm_layers,num_cg_steps,num_samples,num_gl_blocks,device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "NRhWt85g1UoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "gIcAqNDQii-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_H = batch_H.to(device)\n",
        "batch_E = batch_E.to(device)\n",
        "batch_C = batch_C.to(device)"
      ],
      "metadata": {
        "id": "Pu8oVq6Pkhkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_start_index in range(0, data_size, batch_size):\n",
        "    batch_end_index = min(batch_start_index + batch_size, data_size)\n",
        "    batch_X = X[batch_start_index:batch_end_index].detach()\n",
        "    batch_Y = Y_F[batch_start_index:batch_end_index].detach()\n",
        "    batch_X = batch_X.to(device).detach()\n",
        "    batch_Y = batch_Y.to(device).detach()\n",
        "    reconstructed_X = model(batch_Y,batch_E,batch_H,batch_C,(64,64)).to_dense()\n",
        "    loss = loss_fn(reconstructed_X, batch_X.to_dense())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Batch {batch_start_index // batch_size + 1}/{num_batches}, Loss: {loss.detach().item()}\")\n",
        "    del batch_X, batch_Y\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oHoAXruoffJ0",
        "outputId": "3c001f22-6a09-4207-bd73-b4ea0f48c27d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-7c194627a1c2>:5: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
            "  O = torch.stack([torch.sparse.mm(M1[i], M2[i]) for i in range(M1.shape[0])],dim=0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Could not run 'aten::select_backward' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::select_backward' is only available for these backends: [CPU, CUDA, HIP, MPS, IPU, XPU, HPU, VE, MTIA, PrivateUse1, PrivateUse2, PrivateUse3, Meta, FPGA, MAIA, Vulkan, Metal, QuantizedCPU, QuantizedCUDA, QuantizedHIP, QuantizedMPS, QuantizedIPU, QuantizedXPU, QuantizedHPU, QuantizedVE, QuantizedMTIA, QuantizedPrivateUse1, QuantizedPrivateUse2, QuantizedPrivateUse3, QuantizedMeta, CustomRNGKeyId, MkldnnCPU, SparseCsrCPU, SparseCsrCUDA, SparseCsrHIP, SparseCsrMPS, SparseCsrIPU, SparseCsrXPU, SparseCsrHPU, SparseCsrVE, SparseCsrMTIA, SparseCsrPrivateUse1, SparseCsrPrivateUse2, SparseCsrPrivateUse3, SparseCsrMeta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nUndefined: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nCPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nCUDA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nHIP: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nMPS: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nIPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nXPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nHPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nVE: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nMTIA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nPrivateUse1: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nPrivateUse2: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nPrivateUse3: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nMeta: registered at /dev/null:241 [kernel]\nFPGA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nMAIA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nVulkan: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nMetal: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedCPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedCUDA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedHIP: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedMPS: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedIPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedXPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedHPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedVE: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedMTIA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedPrivateUse1: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedPrivateUse2: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedPrivateUse3: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedMeta: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nCustomRNGKeyId: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nMkldnnCPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrCPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrCUDA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrHIP: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrMPS: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrIPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrXPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrHPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrVE: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrMTIA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrPrivateUse1: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrPrivateUse2: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrPrivateUse3: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrMeta: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:503 [backend fallback]\nFunctionalize: registered at /pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:100 [backend fallback]\nAutogradOther: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradCPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradCUDA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradHIP: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradXLA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradMPS: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradIPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradXPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradHPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradVE: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradLazy: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradMTIA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradPrivateUse1: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradPrivateUse2: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradPrivateUse3: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradMeta: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradNestedTensor: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nTracer: registered at /pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:16106 [kernel]\nAutocastCPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:322 [backend fallback]\nAutocastXPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:465 [backend fallback]\nAutocastMPS: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:555 [kernel]\nBatchedNestedTensor: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1079 [kernel]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:499 [backend fallback]\nPreDispatch: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-83662da32274>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructed_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch {batch_start_index // batch_size + 1}/{num_batches}, Loss: {loss.detach().item()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::select_backward' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::select_backward' is only available for these backends: [CPU, CUDA, HIP, MPS, IPU, XPU, HPU, VE, MTIA, PrivateUse1, PrivateUse2, PrivateUse3, Meta, FPGA, MAIA, Vulkan, Metal, QuantizedCPU, QuantizedCUDA, QuantizedHIP, QuantizedMPS, QuantizedIPU, QuantizedXPU, QuantizedHPU, QuantizedVE, QuantizedMTIA, QuantizedPrivateUse1, QuantizedPrivateUse2, QuantizedPrivateUse3, QuantizedMeta, CustomRNGKeyId, MkldnnCPU, SparseCsrCPU, SparseCsrCUDA, SparseCsrHIP, SparseCsrMPS, SparseCsrIPU, SparseCsrXPU, SparseCsrHPU, SparseCsrVE, SparseCsrMTIA, SparseCsrPrivateUse1, SparseCsrPrivateUse2, SparseCsrPrivateUse3, SparseCsrMeta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, Fu...\n\nUndefined: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nCPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nCUDA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nHIP: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nMPS: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nIPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nXPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nHPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nVE: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nMTIA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nPrivateUse1: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nPrivateUse2: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nPrivateUse3: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nMeta: registered at /dev/null:241 [kernel]\nFPGA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nMAIA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nVulkan: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nMetal: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedCPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedCUDA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedHIP: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedMPS: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedIPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedXPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedHPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedVE: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedMTIA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedPrivateUse1: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedPrivateUse2: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedPrivateUse3: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nQuantizedMeta: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nCustomRNGKeyId: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nMkldnnCPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrCPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrCUDA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrHIP: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrMPS: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrIPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrXPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrHPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrVE: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrMTIA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrPrivateUse1: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrPrivateUse2: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrPrivateUse3: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nSparseCsrMeta: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21616 [default backend kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:503 [backend fallback]\nFunctionalize: registered at /pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:100 [backend fallback]\nAutogradOther: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradCPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradCUDA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradHIP: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradXLA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradMPS: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradIPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradXPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradHPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradVE: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradLazy: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradMTIA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradPrivateUse1: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradPrivateUse2: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradPrivateUse3: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradMeta: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nAutogradNestedTensor: registered at /pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:16915 [autograd kernel]\nTracer: registered at /pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:16106 [kernel]\nAutocastCPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:322 [backend fallback]\nAutocastXPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:465 [backend fallback]\nAutocastMPS: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:555 [kernel]\nBatchedNestedTensor: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1079 [kernel]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:499 [backend fallback]\nPreDispatch: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fud84DvVbLhS"
      },
      "outputs": [],
      "source": [
        "R.square().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF7uqZUsbWpM"
      },
      "outputs": [],
      "source": [
        "(X).square().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT5JT_gQdcch"
      },
      "outputs": [],
      "source": [
        "(R - X.to(device)).square().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM6GJb20bYJh"
      },
      "outputs": [],
      "source": [
        "torch.eye(64).view(32,2,64)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}